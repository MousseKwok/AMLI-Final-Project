{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 16 - Strings II - Text processing with NLTK\n",
    "\n",
    "We are going to discuss how to process large text documents using athe [Natural Language Toolkit](http://www.nltk.org) library. \n",
    "\n",
    "We first have to download some data corpora and libraries to use NLTK. Running this block of code *should* pop up a new window with four blue tabs: Collections, Corpora, Models, All Packages. Under Collections, Select the entry with \"book\" in the Identifier column and select download. Once the status \"Finished downloading collection 'book'.\" prints in the grey bar at the bottom, you can close this pop-up.\n",
    "\n",
    "![](http://www.nltk.org/images/nltk-downloader.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should only need to do the download step once. In the future, you can start from the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import text1, text2, text3, text4, text5, text6, text7, text8, text9\n",
    "text_list = [text1, text2, text3, text4, text5, text6, text7, text8, text9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also download text from the web and load it into a NLTK Text object. Let's get something from [Project Gutenberg's Top 100 list](https://www.gutenberg.org/browse/scores/top), like Charles Dickens's \"A Tale of Two Cities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from recitation\n",
    "re.search(r\"(I. The Period.*)End of the Project Gutenberg EBook of A Tale of Two Cities, by Charles Dickens.*$\",\n",
    "                     atotc_raw, re.S) #it matches all, it'll make the .*  match any character/ will match anything\n",
    "#until it sees the specific lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from recitation\n",
    "re.search(r\"(I. The Period.*)End of the Project Gutenberg EBook of A Tale of Two Cities, by Charles Dickens.*$\",\n",
    "                     atotc_raw, re.S).groups() #groups if it matched a bunch of things, it would match any of them...? \n",
    "#a strip() we can pull that out as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text file from the web\n",
    "atotc_raw = requests.get('https://www.gutenberg.org/files/98/98-0.txt').text\n",
    "\n",
    "# Write a regular expression to get everything between these two lines of text\n",
    "atotc_core = re.search(r\"(I. The Period.*)End of the Project Gutenberg EBook of A Tale of Two Cities, by Charles Dickens.*$\",\n",
    "                     atotc_raw, re.S).groups()[0].strip()\n",
    "\n",
    "# Tokenize the raw text\n",
    "atotc_tokenized = nltk.word_tokenize(atotc_core)\n",
    "\n",
    "# Convert to a nltk Text object\n",
    "text10 = nltk.Text(atotc_tokenized)\n",
    "\n",
    "# Give the text a formal name, like the other text objects\n",
    "text10.name = 'A Tale of Two Cities by Charles Dickens 1859'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this text object to the text_list\n",
    "text_list.append(text10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long is each body of text? We can use `len` on a `Text` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in text_list:\n",
    "    print(\"{0} has {1:,} words\\n\".format(t.name,len(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique words in each text document? We can call `set` on a Text object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in text_list:\n",
    "    print(\"{0} has {1:,} unique words\\n\".format(t.name,len(set(t))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function that measures the [lexical diversity](https://en.wikipedia.org/wiki/Lexical_diversity) of the text by computing the number of unique words as a percentage of the total number of words. If each word was used only once, then the richness would be 100% but if the same word was repeated the entire length of the document, then the richness would be 0%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text))/len(text)\n",
    "\n",
    "for t in text_list:\n",
    "    print(\"{0} has a lexical diversity of {1:.2}\\n\".format(t.name,lexical_diversity(t)))\n",
    "    #can change the {1:.1%}\\n to change the decimal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the longest words in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_word(text):\n",
    "    longest = ''\n",
    "    for word in set(text):\n",
    "        if len(word) > len(longest):\n",
    "            longest = word\n",
    "    return longest\n",
    "\n",
    "for t in text_list:\n",
    "    print(\"The longest word in {0} is: {1}\\n\".format(t.name,find_longest_word(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also measure the frequency distribution of how often a word is used in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t = text1 #moby dick \n",
    "fdist_text1 = nltk.FreqDist(_t)\n",
    "fdist_text1.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the distribution of how often words occur in the corpus. We get an interesting pattern called the [Zipf distribution](https://en.wikipedia.org/wiki/Zipf%27s_law). There are many words that occur only once (upper left) and single words that occur thousands of times (lower right) but the pattern follows a consistent log-linear pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_text1 = Counter(fdist_text1.values())\n",
    "\n",
    "f,ax = plt.subplots(1,1)\n",
    "ax.scatter(list(counter_text1.keys()),list(counter_text1.values()))\n",
    "ax.set_ylim((1e-1,1e5))\n",
    "ax.set_xscale('log') #puts the scales on powers of 10- without these, it's pretty boring looking \n",
    "ax.set_yscale('log')\n",
    "ax.set_title(_t.name)\n",
    "ax.set_xlabel('Number of occurrences')\n",
    "ax.set_ylabel('Number of words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important part of processing natural language data is normalizing this data by removing variations in the text that the computer naively thinks are different entities but humans recognize as being the same. There are several steps to this including case adjustment and stemming/lemmatization.\n",
    "\n",
    "In the case of case adjustment, it turns out several of the different \"words\" in the corpus are actually the same, but because they have different capitalizations, they're counted as different unique words. Explore how many five-letter words are the same, just with different capitalizations.\n",
    "\n",
    "![](http://www.nltk.org/images/pipeline1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmenting\n",
    "\n",
    "A novel can be represented as a single large string, but this huge string isn't very helpful for analyzing features of the text until the string is segmented into sentences or \"tokens\", which include words but also hyphenated phrases or contractions (\"aren't\", \"doesn't\", *etc*.)\n",
    "\n",
    "There are a variety of different segmentation/tokenization strategies (with different tradeoffs) and corresponding methods implemented in NLTK.\n",
    "\n",
    "If we wanted to get all the sentences in a string, we could naively split the string on a period and whitespace using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atotc_core.split('. ')[2:5]\n",
    "#mrs. Southcott messes it up so this isn't the best way to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splitting method only uses space characters, but not newline characters `\\r\\n` to split, so it misses several sentences. We could use a regular expression to split on periods and white spaces too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r'\\.\\s+',atotc_core)[2:10] #can change to [3:8] \n",
    "#split on a period and a white space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this sentance tokenizing fails for a phrase like \"Mrs. Southcott had recently attained her five and twentieth blessed birthday...\" into two sentences, when it should be one.\n",
    "\n",
    "NLTK has more specialized sentence tokenizers that deal with these kinds of cases. You should probably use these instead of trying to make your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.sent_tokenize(atotc_core)[2:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenizing\n",
    "We may care less about sentences and more about individual words. Again, we could employ a naive approach of splitting on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_tokens = atotc_core.split(' ')\n",
    "space_tokens[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this misses newline separators, so we might think we could use regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tokens = re.split(r'\\s+',atotc_core)\n",
    "re_tokens[0:50]\n",
    "#times has a comma on the inside, so Python will take it differently than 'times' without a comma "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear we want to separate words based on other punctuation as well so that \"Darkness,\" and \"Darkness\" aren't treated like separate words. Again, NLTK has a variety of methods for doing word tokenization more intelligently.\n",
    "\n",
    "`word_tokenize` is probably the easiest-to-recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_tokens = nltk.word_tokenize(atotc_core)\n",
    "wt_tokens[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are others like `wordpunct_tokenize` tha makes different assumptions about the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt_tokens = nltk.wordpunct_tokenize(atotc_core)\n",
    "wpt_tokens[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or `Toktok` is still another word tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toktok = nltk.ToktokTokenizer()\n",
    "ttt_tokens = toktok.tokenize(atotc_core)\n",
    "ttt_tokens[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these different methods returns a different word count based on their different assumptions about word boundaries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,tokenlist in zip(['space_split','re_tokenizer','word_tokenizer','wordpunct_tokenizer','toktok_tokenizer'],[space_tokens,re_tokens,wt_tokens,wpt_tokens,ttt_tokens]):\n",
    "    print(\"{0:>20}: {1:,} words\".format(name,len(tokenlist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed cases\n",
    "\n",
    "Remember that strings of different cases (capitalizations) are treated as different words: \"young\" and \"Young\" are not the same. An important part of text processing is to remove un-needed variation, and mixed cases are variation we generally don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_letter_words = [word for word in set(text1) if len(word) == 5]\n",
    "\n",
    "print(\"There are {0:,} five-letter words in the corpus.\".format(len(five_letter_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_case_tokens = []\n",
    "\n",
    "for word1 in five_letter_words:\n",
    "    for word2 in five_letter_words:\n",
    "        if word1.lower() == word2.lower() and word1 != word2:\n",
    "            mixed_case_tokens.append((word1,word2))\n",
    "\n",
    "print(\"There are {0:,} five-letter words in the corpus that are the same but have different cases.\".format(len(mixed_case_tokens)))\n",
    "mixed_case_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the number of words in the document change after applying `.lower()` to everything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_lowered = [i.lower() for i in text1.tokens]\n",
    "print(\"There are {0:,} unique words in text1 before lowering and {1:,} after lowering\".format(len(set(text1)),len(set(text1_lowered))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "\n",
    "English, like many languages, repeats many words in typical language that don't always convey a lot of information by themselves. When we do text processing, we should make sure to remove these \"stop words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_text10 = nltk.FreqDist(text10)\n",
    "fdist_text10.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK helpfully has a list of stopwords in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "english_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(english_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `string` module's \"punctuation\" attribute as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(string.punctuation)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine them so get a list of `all_stopwords` that we can ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = english_stopwords + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a list comprehension to exclude the words in this stopword list from analysis while also gives each word similar cases. This is not perfect, but an improvement over what we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text10_no_stopwords = [word.lower() for word in text10 if word.lower() not in all_stopwords]\n",
    "fdist_text10_no_stopwords = nltk.FreqDist(text10_no_stopwords)\n",
    "fdist_text10_no_stopwords.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "Another problem with natural language text is plural (dogs vs. dog) and possessive (dog's vs. dog) forms, verb conjugations (walk, walks, walked, walking), and contractions (they're) are also counted as unique words even if the underlying concepts are similar. Extracting [word stems](https://en.wikipedia.org/wiki/Word_stem) means removing prefixes and affixes that result in a new token, but not a significantly new meaning.\n",
    "\n",
    "We can use a variety of [stemming](https://en.wikipedia.org/wiki/Stemming) and [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) tools in NLTK to try to recover unique words stripped of any prefixes or suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.lower() for t in text1.tokens[10:50] if len(t) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "[porter.stem(t.lower()) for t in text1.tokens[10:50] if len(t) > 2]\n",
    "#lets us clean up the data so we can remove some of the variation \n",
    "#stemming is fast, but is weird because it just takes off the prefix or suffix of words sometimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After stemming the words in `text1`, how many unique words remain? \n",
    "\n",
    "Nearly half of the words in *Moby Dick* that were initially counted as unique were actually duplicates of other words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_lowered_stemmed = set()\n",
    "\n",
    "for t in set(text1):\n",
    "    t_lower = t.lower()\n",
    "    t_stemmed = porter.stem(t_lower)\n",
    "    text1_lowered_stemmed.add(t_stemmed)\n",
    "    \n",
    "print(\"There are {0:,} unique words in text1 before and {1:,} after lowering and stemming\".format(len(set(text1)),len(set(text1_lowered_stemmed))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a bit smarter about removing letters: it checks if the word is a plural, conjugation, etc. of another word and them \"stems\" it down to the root word only if in the dictionary. These lookups are expensive in comparision to basically slicing characters off a list like stemming, but results in better quality — but far from perfect — results. For example, \"supplied\" should have been reduced to \"supply\" and \"dusting\" and \"dust\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "[wnl.lemmatize(t.lower()) for t in text1.tokens[10:50] if len(t) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After lemmatizing the words in `text1`, how many unique words remain? Lemmatizing isn't as aggressive as stemming, but there's still a 25% reduction in the total number of unique words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_lowered_lemmatized = set()\n",
    "\n",
    "for t in set(text1):\n",
    "    t_lower = t.lower()\n",
    "    t_lemmatized = wnl.lemmatize(t_lower)\n",
    "    text1_lowered_lemmatized.add(t_lemmatized)\n",
    "    \n",
    "print(\"There are {0:,} unique words in text1 before and {1:,} after lowering and stemming\".format(len(set(text1)),len(set(text1_lowered_lemmatized))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Download the raw text for \"The Importance of Being Earnest\" from [Project Gutenberg](http://www.gutenberg.org/cache/epub/844/pg844.txt) and save it as `tiobe_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiobe_raw = requests.get('http://www.gutenberg.org/cache/epub/844/pg844.txt').text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the regular expression used previously to get all the text between \"FIRST ACT\" and \"END OF THE PROJECT GUTENBERG EBOOK THE IMPORTANCE OF BEING EARNEST\" and save it as `tiobe_core`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiobe_core = re.search(r\"(FIRST ACT.*)END OF THE PROJECT GUTENBERG EBOOK THE IMPORTANCE OF BEING EARNEST,BY OSCAR WILDE.*$\",\n",
    "                    tiobe_raw, re.S).groups()[0].strip()\n",
    "tiobe_core[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the text processing steps into one function that accepts a string like `tiobe_core` and returns a list of lower-cased, stopword-removed, and lemmatized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = nltk.corpus.stopwords.words('english') + list(string.punctuation) + ['“','”','’','--']\n",
    "\n",
    "def text_cleaner(s):\n",
    "    # Lower-case everything in the string\n",
    "    lower_s = s.lower() \n",
    "    \n",
    "    # Tokenize the string\n",
    "    tokenized_s = nltk.word_tokenize(lower_s)\n",
    "    \n",
    "    # Remove the stopwords from the list of tokens\n",
    "    no_stopwords = [token for token in tokenized_s if token not in all_stopwords]\n",
    "    #we're going token by token\n",
    "    \n",
    "    # lemmatize each token\n",
    "    cleaned_tokens = [wnl.lemmatize(token) for token in no_stopwords] \n",
    "    \n",
    "    # Return the cleaned_tokens\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function on `tiobe_core`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiobe_tokens = text_cleaner(tibobe_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"There are {0:,} toekns and {1:,} unique tokens in the corpus\".format(len)(tiobe_tokens),len(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `Counter` function on `cleaned_tokens` to count how often each cleaned token occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiobe_token_frequencies = Counter(tiobe_tokens)\n",
    "most_frequent_tokens = sorted(tiobe_frequencies.items(),key=lambda x:x[1],reverse=True)\n",
    "most_frequent_tokens[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the word frequency distribution for the 50-most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [word for (word,count) in most_frequent_tokens[:50]]\n",
    "y = [count for (word,count) in most_frequent_tokens[:50]]\n",
    "\n",
    "f,ax = plt.subplots(1,1,figsize=(12,6))\n",
    "ax.plot(y,lw=3)\n",
    "ax.set_xticks(range(len(x)))\n",
    "ax.set_xticklabels(x,rotation=90)\n",
    "ax.set_xlabel('Tokens')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a distribution of the frequencies themselves. This is a classic distribution found throughout information science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_frequencies = Counter(tiobe_token_frequencies.values())\n",
    "\n",
    "f,ax = plt.subplots(1,1,figsize=(12,6))\n",
    "ax.scatter(frequency_frequencies.keys(),frequency_frequencies.values())\n",
    "#ax.scatter(frequency_frequencies.values(),frequency_frequencies.keys())\n",
    "ax.set_xlabel('Number of occurrences')\n",
    "ax.set_ylabel('Number of words')\n",
    "ax.set_xscale('symlog')\n",
    "ax.set_yscale('symlog')\n",
    "ax.grid(True)\n",
    "ax.set_xlim((0,1e3))\n",
    "ax.set_ylim((0,1e4))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
