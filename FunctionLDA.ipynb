{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FunctionLDA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbaeAOu4XmKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_name = 'reviews_pittsburgh.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7TpVKeITKFY",
        "colab_type": "code",
        "outputId": "9ddd26a8-f7be-4dc0-bfb7-b546d143f168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# import dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from gensim.models import LdaModel\n",
        "from gensim import models, corpora, similarities\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import time\n",
        "from nltk import FreqDist\n",
        "from gensim.models import HdpModel\n",
        "from scipy.stats import entropy\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "print('Downloads Complete')\n",
        "\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Downloads Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbTRSwxgTvUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initial_clean(text):\n",
        "    \"\"\"\n",
        "    Function to clean text of websites, email addresess and any punctuation\n",
        "    We also lower case the text\n",
        "    \"\"\"\n",
        "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
        "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
        "    text = text.lower() # lower case the text\n",
        "    text = nltk.word_tokenize(text)\n",
        "    return text\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    \"\"\"\n",
        "    Function that removes all stopwords from text\n",
        "    \"\"\"\n",
        "    return [word for word in text if word not in stop_words]\n",
        "\n",
        "def pos(word):\n",
        "    return nltk.pos_tag([word])[0][1]\n",
        "\n",
        "bad_pos = ['JJ']\n",
        "def remove_bad_pos(text):\n",
        "    \"\"\"\n",
        "    Function that removes all adjectives from text\n",
        "    \"\"\"\n",
        "    return [word for word in text if pos(word) not in bad_pos]\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    \"\"\"\n",
        "    Function to stem words, so plural and singular are treated the same\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = [stemmer.stem(word) for word in text]\n",
        "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
        "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
        "        pass\n",
        "    return text\n",
        "\n",
        "def apply_all(text):\n",
        "    \"\"\"\n",
        "    This function applies all the functions above into one\n",
        "    \"\"\"\n",
        "    #return remove_stop_words(initial_clean(text))\n",
        "    return stem_words(remove_bad_pos(remove_stop_words(initial_clean(text))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wpMk8BETxnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_lda(data, chunksize):\n",
        "    \"\"\"\n",
        "    This function trains the lda model\n",
        "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
        "    \"\"\"\n",
        "    dictionary = corpora.Dictionary(data['tokenized'])\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n",
        "    t1 = time.time()\n",
        "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
        "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
        "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
        "                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=1)\n",
        "    t2 = time.time()\n",
        "    print(\"Time to train LDA model on \", len(df), \"businesses: \", (t2-t1)/60, \"min\")\n",
        "    return dictionary,corpus,lda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOMcVS6WTeJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_hdp(data):\n",
        "    \"\"\"\n",
        "    This function trains the lda model\n",
        "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
        "    \"\"\"\n",
        "    dictionary = corpora.Dictionary(data['tokenized'])\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n",
        "    t1 = time.time()\n",
        "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
        "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
        "    hdp = HdpModel(corpus=corpus, id2word=dictionary)\n",
        "    t2 = time.time()\n",
        "    print(\"Time to train HDP model on \", len(df), \"businesses: \", (t2-t1)/60, \"min\")\n",
        "    return hdp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc4I6vq5T1N6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jensen_shannon(query, matrix):\n",
        "    \"\"\"\n",
        "    This function implements a Jensen-Shannon similarity\n",
        "    between the input query (an LDA topic distribution for a document)\n",
        "    and the entire corpus of topic distributions.\n",
        "    It returns an array of length M where M is the number of documents in the corpus\n",
        "    \"\"\"\n",
        "    # lets keep with the p,q notation above\n",
        "    p = query[None,:].T # take transpose\n",
        "    q = matrix.T # transpose matrix\n",
        "    m = 0.5*(p + q)\n",
        "    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8qJicnwT5Ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_most_similar_documents(query,matrix,k=10):\n",
        "    \"\"\"\n",
        "    This function implements the Jensen-Shannon distance above\n",
        "    and retruns the top k indices of the smallest jensen shannon distances\n",
        "    \"\"\"\n",
        "    sims = jensen_shannon(query,matrix) # list of jensen shannon distances\n",
        "    return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdR22g-xTOCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# format the columns\n",
        "df = pd.read_csv(csv_name)\n",
        "df = df.groupby(['name'])['text'].apply(' '.join).reset_index()\n",
        "df = df[df['text'].map(type) == str]\n",
        "df.dropna(axis=0, inplace=True, subset=['text'])\n",
        "\n",
        "# preprocess the text and business name and create new column \"tokenized\"\n",
        "t1 = time.time()\n",
        "df['tokenized'] = df['text'].apply(apply_all)\n",
        "t2 = time.time()\n",
        "print(\"Time to clean and tokenize\", len(df), \"businesses' reviews:\", (t2-t1)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EiI6qhWCC7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# config\n",
        "k = 15000\n",
        "num_topics = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXAL6BaNTf9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first get a list of all words\n",
        "all_words = [word for item in list(df['tokenized']) for word in item]\n",
        "\n",
        "# use nltk fdist to get a frequency distribution of all words\n",
        "fdist = FreqDist(all_words)\n",
        "\n",
        "# define a function only to keep words in the top k words\n",
        "top_k_words,_ = zip(*fdist.most_common(k))\n",
        "top_k_words = set(top_k_words)\n",
        "def keep_top_k_words(text):\n",
        "    return [word for word in text if word in top_k_words]\n",
        "df['tokenized'] = df['tokenized'].apply(keep_top_k_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SZPV60mSnXH",
        "colab_type": "code",
        "outputId": "07d1ebc3-dc4e-42f7-a5f7-90d34344a586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# train the topic model\n",
        "hdp = train_hdp(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to train HDP model on  422 businesses:  0.46153692801793417 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCkdVcHSVM-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_info = hdp.print_topics(num_topics=20, num_words=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiIUu0K5VRmp",
        "colab_type": "code",
        "outputId": "0b76645f-6026-4bcf-832b-44cbad5989d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "topic_info"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.013*food + 0.011*good + 0.010*place + 0.008*order + 0.008*great + 0.007*like + 0.007*time + 0.006*go + 0.006*get + 0.006*one'),\n",
              " (1,\n",
              "  '0.013*food + 0.011*place + 0.011*good + 0.008*order + 0.008*great + 0.007*like + 0.007*time + 0.007*get + 0.007*go + 0.006*one'),\n",
              " (2,\n",
              "  '0.011*food + 0.010*good + 0.009*place + 0.008*burger + 0.008*great + 0.008*order + 0.007*like + 0.006*one + 0.006*time + 0.006*go'),\n",
              " (3,\n",
              "  '0.014*food + 0.010*place + 0.010*good + 0.008*order + 0.007*great + 0.007*like + 0.006*time + 0.006*go + 0.006*get + 0.006*tri'),\n",
              " (4,\n",
              "  '0.013*food + 0.010*good + 0.009*place + 0.009*order + 0.007*great + 0.007*time + 0.007*servic + 0.006*restaur + 0.006*like + 0.006*go'),\n",
              " (5,\n",
              "  '0.012*food + 0.011*place + 0.011*good + 0.009*beer + 0.007*order + 0.007*great + 0.007*like + 0.007*pizza + 0.007*get + 0.006*go'),\n",
              " (6,\n",
              "  '0.012*food + 0.009*good + 0.008*place + 0.007*get + 0.007*airport + 0.007*great + 0.006*time + 0.006*like + 0.006*go + 0.006*one'),\n",
              " (7,\n",
              "  '0.010*place + 0.010*good + 0.010*food + 0.008*sushi + 0.007*order + 0.007*chicken + 0.007*great + 0.006*tri + 0.006*roll + 0.006*like'),\n",
              " (8,\n",
              "  '0.015*pizza + 0.008*good + 0.008*food + 0.007*place + 0.007*great + 0.006*order + 0.006*like + 0.006*gelato + 0.006*tri + 0.005*time'),\n",
              " (9,\n",
              "  '0.011*fish + 0.010*place + 0.009*sushi + 0.009*order + 0.009*food + 0.008*good + 0.006*great + 0.005*roll + 0.005*like + 0.005*fresh'),\n",
              " (10,\n",
              "  '0.010*place + 0.007*pizza + 0.007*good + 0.007*like + 0.006*food + 0.006*great + 0.006*sushi + 0.006*get + 0.005*time + 0.005*go'),\n",
              " (11,\n",
              "  '0.019*tea + 0.011*place + 0.009*pho + 0.007*good + 0.006*like + 0.006*order + 0.006*chicken + 0.005*bubbl + 0.005*get + 0.005*tri'),\n",
              " (12,\n",
              "  '0.014*hotel + 0.013*room + 0.007*stay + 0.005*servic + 0.005*great + 0.005*one + 0.004*like + 0.004*good + 0.004*time + 0.004*night'),\n",
              " (13,\n",
              "  '0.013*pho + 0.009*place + 0.009*good + 0.008*food + 0.007*burger + 0.006*order + 0.006*like + 0.005*great + 0.005*time + 0.004*realli'),\n",
              " (14,\n",
              "  '0.011*food + 0.009*place + 0.007*good + 0.006*great + 0.006*order + 0.005*breakfast + 0.005*get + 0.005*time + 0.004*pittsburgh + 0.004*diner'),\n",
              " (15,\n",
              "  '0.016*ice + 0.016*cream + 0.011*flavor + 0.007*place + 0.006*cone + 0.006*good + 0.006*like + 0.005*tri + 0.005*waffl + 0.005*order'),\n",
              " (16,\n",
              "  '0.035*pizza + 0.009*chees + 0.008*place + 0.008*sauc + 0.008*good + 0.008*best + 0.006*crust + 0.006*pittsburgh + 0.006*like + 0.005*fiori'),\n",
              " (17,\n",
              "  '0.011*crepe + 0.008*place + 0.008*good + 0.007*food + 0.006*order + 0.005*great + 0.005*servic + 0.004*burger + 0.004*time + 0.004*wait'),\n",
              " (18,\n",
              "  '0.008*coffe + 0.008*place + 0.007*great + 0.006*drink + 0.006*food + 0.005*good + 0.005*get + 0.005*bar + 0.004*like + 0.004*go'),\n",
              " (19,\n",
              "  '0.012*food + 0.008*indian + 0.006*sauc + 0.006*order + 0.005*place + 0.005*bowl + 0.005*tri + 0.005*good + 0.005*masala + 0.004*also')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YnYdjQzd2ax",
        "colab_type": "code",
        "outputId": "dd90af0c-465a-4561-9003-833863ee98bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# train the topic model\n",
        "dictionary,corpus,lda = train_lda(df, 211)\n",
        "\n",
        "# get the topic distribution\n",
        "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])\n",
        "\n",
        "query = pd.read_csv('reviews_pittsburgh.csv')\n",
        "query = query.loc[query['name']=='Altius']\n",
        "query = query.groupby(['name'])['text'].apply(' '.join).reset_index()\n",
        "query = query[query['text'].map(type) == str]\n",
        "query.dropna(axis=0, inplace=True, subset=['text'])\n",
        "query['tokenized'] = query['text'].apply(apply_all)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
            "  diff = np.log(self.expElogbeta)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to train LDA model on  422 businesses:  0.21156521638234457 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zONtAIhpfIV_",
        "colab_type": "code",
        "outputId": "8dd228b1-033c-4404-ef16-11e981eb4a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# get the ids of the most similar businesses\n",
        "new_bow = dictionary.doc2bow(query.iloc[0,2])\n",
        "new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
        "most_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)\n",
        "\n",
        "# print the results\n",
        "most_similar_df = df[df.index.isin(most_sim_ids)]\n",
        "print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similar to \"Altius\": \n",
            "0                             Altius\n",
            "1    Cioppino Restaurant & Cigar Bar\n",
            "2        Eddie Merlot's - Pittsburgh\n",
            "3            Eddie V's Prime Seafood\n",
            "4                 Habitat Restaurant\n",
            "5                            Le Mont\n",
            "6           Monterey Bay Fish Grotto\n",
            "7                      Sonoma Grille\n",
            "8                 The Capital Grille\n",
            "9                            Vue 412\n",
            "Name: name, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKyPIq2evIwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxVIPQXVu4xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(doc_topic_dist,open(\"processor.pkl\",\"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDBy4J73vH3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnarrrSWvpHM",
        "colab_type": "code",
        "outputId": "fd4437fd-daba-4e9e-a530-ebac794d350b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
        "\n",
        "# print the results\n",
        "most_similar_df = df[df.index.isin(most_sim_ids)]\n",
        "print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similar to \"Altius\": \n",
            "0                         Altius\n",
            "1                      Bistro 19\n",
            "2    Eddie Merlot's - Pittsburgh\n",
            "3                         Eleven\n",
            "4                        Le Mont\n",
            "5        Morton's The Steakhouse\n",
            "6                          Spoon\n",
            "7             The Capital Grille\n",
            "8      Toast! Kitchen & Wine Bar\n",
            "9                        Vue 412\n",
            "Name: name, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OTkrbs7zvoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_query(preprocessed, query):\n",
        "  # SQL to pandas DataFrame w/ query\n",
        "  query = query.groupby(['name'])['text'].apply(' '.join).reset_index()\n",
        "  query = query[query['text'].map(type) == str]\n",
        "  query.dropna(axis=0, inplace=True, subset=['text'])\n",
        "  query['tokenized'] = query['text'].apply(apply_all)\n",
        "  \n",
        "  # read the cached pickle\n",
        "  preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
        "  \n",
        "  # get the ids of the most similar businesses\n",
        "  new_bow = dictionary.doc2bow(query.iloc[0,2])\n",
        "  new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
        "  most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
        "\n",
        "  # print the results\n",
        "  most_similar_df = df[df.index.isin(most_sim_ids)]\n",
        "  print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}