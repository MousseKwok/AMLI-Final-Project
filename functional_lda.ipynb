{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7TpVKeITKFY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/xijieguo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xijieguo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/xijieguo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import os\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, jensen_shannon\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print('Downloads Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbTRSwxgTvUC"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def initial_clean(text):\n",
    "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = text.lower() # lower case the text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "def pos(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "informative_pos = ('JJ','VB', 'NN','RBS','VBP','IN','RBR','JJR','JJS','PDT','RP','UH','FW','NNS','VBN','VBG')\n",
    "\n",
    "def remove_uninformative_pos(text):\n",
    "    tagged_words = nltk.pos_tag(text)\n",
    "    return [word for word, tag in tagged_words if tag in informative_pos]\n",
    "  \n",
    "clutter = ['food','place','good','order','great','like',\n",
    "           'service','time','go','ordered','get','love',\n",
    "           'best','come','eat','dont','tried','try','ask',\n",
    "           'nice','restaurant','ive','im','didnt']\n",
    "\n",
    "def remove_garbage(text):\n",
    "    return [word for word in text if word not in clutter]\n",
    "\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
    "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(remove_garbage(remove_uninformative_pos(remove_stop_words(initial_clean(text)))))\n",
    "\n",
    "def get_top_k_words(df, k = 10000):\n",
    "    # first get a list of all words\n",
    "    all_words = [word for item in list(df['tokenized']) for word in item]\n",
    "    \n",
    "    # use nltk fdist to get a frequency distribution of all words\n",
    "    fdist = FreqDist(all_words)\n",
    "    \n",
    "    # define a function only to keep words in the top k words\n",
    "    top_k_words, _ = zip(*fdist.most_common(k))\n",
    "    top_k_words = set(top_k_words)\n",
    "    \n",
    "    return top_k_words\n",
    "\n",
    "def keep_top_k_words(text, *top_k_words):\n",
    "    return [word for word in text if word in top_k_words]\n",
    "\n",
    "def transform_dataset(df):\n",
    "    # format the columns\n",
    "    df = df.groupby(['business_id', 'name'])['text'].apply(' '.join).reset_index()\n",
    "    df = df[df['text'].map(type) == str]\n",
    "    df.dropna(axis=0, inplace=True, subset=['text'])\n",
    "    return df\n",
    "\n",
    "def gen_tokenized_column(df):\n",
    "    # preprocess the text and business name and create new column \"tokenized\"\n",
    "    df['tokenized'] = df['text'].apply(apply_all)\n",
    "    top_k_words = get_top_k_words(df)\n",
    "    df['tokenized'] = df['tokenized'].apply(keep_top_k_words, args=(top_k_words))\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(df):\n",
    "    t1 = time.time()\n",
    "    preprocessed_df = gen_tokenized_column(transform_dataset(df))\n",
    "    t2 = time.time()\n",
    "    print(\"Time to clean and tokenize\", len(df), \"businesses' reviews:\", (t2-t1)/60, \"min\")\n",
    "    return preprocessed_df\n",
    "    \n",
    "def get_coherence_score(model, texts, dictionary):\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "def get_dictionary_corpus(data, no_below=20, no_above=0.5):\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def get_perplexity(model, corpus):\n",
    "    # a measure of how good the model is; lower the better\n",
    "    return lda_model.log_perplexity(corpus)\n",
    "\n",
    "def train_lda(corpus, id2word, chunksize=2000, num_topics=12, alpha='auto', eta='auto', passes=1, iterations=50,\n",
    "              minimum_probability=0.01, eval_every=10, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, alpha=alpha, eta=eta, \n",
    "                   chunksize=chunksize, minimum_probability=minimum_probability, passes=passes, \n",
    "                   iterations=iterations, eval_every=eval_every, random_state=random_state)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lda\n",
    "\n",
    "def train_hdp(corpus, id2word, chunksize=2000, T=150, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the hdp model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    hdp = HdpModel(corpus=corpus, id2word=id2word, T=T, chunksize=chunksize, random_state=random_state)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train HDP model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return hdp\n",
    "\n",
    "def train_lsi(corpus, id2word, num_topics=12, chunksize=2000, onepass=True, power_iters=2, extra_samples=100):\n",
    "    \"\"\"\n",
    "    This function trains the lsi model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lsi = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, chunksize=chunksize)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LSI model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lsi\n",
    "\n",
    "def get_most_similar_documents(query, corpus, dictionary, k=10):\n",
    "    distances = []\n",
    "    for c in corpus:\n",
    "        distances.append(kullback_leibler(query, c, num_features=len(dictionary)))\n",
    "    \n",
    "    indices = np.array(distances).argsort()[:k]\n",
    "    return indices\n",
    "\n",
    "def get_topic_dist(model, corpus):\n",
    "    doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in model[corpus]])\n",
    "    return doc_topic_dist\n",
    "\n",
    "def get_most_similar_businesses(query_data, corpus, dictionary, model):\n",
    "    query_bow = dictionary.doc2bow(query_data)\n",
    "    most_sim_ids = get_most_similar_documents(model[query_bow], model[corpus], dictionary)\n",
    "    return most_sim_ids \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train HDP model on businesses:  0.007100733121236166 min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>address</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>IXUwuNjy707wJNW2U4sRVg</td>\n",
       "      <td>Alessia's Ristorante Italiano</td>\n",
       "      <td>33.436137</td>\n",
       "      <td>-111.716897</td>\n",
       "      <td>5251 E Brown Rd</td>\n",
       "      <td>4.5</td>\n",
       "      <td>I like Italian food. It's probably the only fo...</td>\n",
       "      <td>[like, italian, food, food, take, home, left, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>pkEYhukWbbnvfhcx5B_qmg</td>\n",
       "      <td>Seafood Market and Restaurant</td>\n",
       "      <td>33.379451</td>\n",
       "      <td>-111.757781</td>\n",
       "      <td>3406 E Baseline Rd</td>\n",
       "      <td>3.0</td>\n",
       "      <td>We moved to AZ a few years ago and don't know ...</td>\n",
       "      <td>[move, year, know, mani, seafood, restaur, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>ShYv4mmgNwrStaFnje3uZQ</td>\n",
       "      <td>SaDec Bistro</td>\n",
       "      <td>33.321093</td>\n",
       "      <td>-111.685216</td>\n",
       "      <td>5221 S Power Rd, Are 104</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I came here for lunch one day, and tried the P...</td>\n",
       "      <td>[lunch, day, pho, bun, bo, hue, substitut, egg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>_myTPlWa8QRN5eavs-hNPg</td>\n",
       "      <td>Pacino's Italian</td>\n",
       "      <td>33.466853</td>\n",
       "      <td>-111.682591</td>\n",
       "      <td>2831 N Power Rd</td>\n",
       "      <td>3.5</td>\n",
       "      <td>We went last Saturday night and it was awesome...</td>\n",
       "      <td>[last, saturday, night, awesom, place, live, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>NlymcSxDnjtCLyGvmMoyFg</td>\n",
       "      <td>Aloha Kitchen</td>\n",
       "      <td>33.360701</td>\n",
       "      <td>-111.859793</td>\n",
       "      <td>2950 S Alma School Rd, Ste 12</td>\n",
       "      <td>4.5</td>\n",
       "      <td>So delicious! My family has been coming here f...</td>\n",
       "      <td>[delici, famili, come, year, husband, favorit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3dw6xhzG08htY5HcL2OjeA</td>\n",
       "      <td>Angry Crab Shack</td>\n",
       "      <td>33.364147</td>\n",
       "      <td>-111.655685</td>\n",
       "      <td>8253 E Guadalupe Rd</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Been here at least a dozen times, while the se...</td>\n",
       "      <td>[least, dozen, time, servic, poor, food, good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>HnpV9DkAV3dF88XgalLFbQ</td>\n",
       "      <td>Pho Viet Nam #2</td>\n",
       "      <td>33.434528</td>\n",
       "      <td>-111.719609</td>\n",
       "      <td>1042 N Higley Rd, Ste 103</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Very nice restaurant, service is always great!...</td>\n",
       "      <td>[nice, restaur, servic, great, choic, asian, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9_HgBRUXUg_vKjTT-GSulA</td>\n",
       "      <td>Golden Hawaiian BBQ</td>\n",
       "      <td>33.393803</td>\n",
       "      <td>-111.868805</td>\n",
       "      <td>1720 W Southern Ave, Ste C3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I had always heard great things and finally de...</td>\n",
       "      <td>[great, thing, tri, chicken, katsu, amaz, gold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>VxgjMl8tQrT4idJW_z8daA</td>\n",
       "      <td>Carrabba's Italian Grill</td>\n",
       "      <td>33.384757</td>\n",
       "      <td>-111.679534</td>\n",
       "      <td>1740 S Clearview Ave</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yay for Carrabbas...after a bad experience wit...</td>\n",
       "      <td>[bad, experi, tri, get, outback, next, door, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>DgZ-pZUo3drzpiCDlDr9IQ</td>\n",
       "      <td>unPhogettable</td>\n",
       "      <td>33.412909</td>\n",
       "      <td>-111.875826</td>\n",
       "      <td>66 S Dobson Rd, Ste 138</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I ordered a rare beef pho with added beef ball...</td>\n",
       "      <td>[order, rare, beef, pho, beef, ball, beef, che...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id                           name   latitude  \\\n",
       "89   IXUwuNjy707wJNW2U4sRVg  Alessia's Ristorante Italiano  33.436137   \n",
       "225  pkEYhukWbbnvfhcx5B_qmg  Seafood Market and Restaurant  33.379451   \n",
       "128  ShYv4mmgNwrStaFnje3uZQ                   SaDec Bistro  33.321093   \n",
       "160  _myTPlWa8QRN5eavs-hNPg               Pacino's Italian  33.466853   \n",
       "105  NlymcSxDnjtCLyGvmMoyFg                  Aloha Kitchen  33.360701   \n",
       "25   3dw6xhzG08htY5HcL2OjeA               Angry Crab Shack  33.364147   \n",
       "86   HnpV9DkAV3dF88XgalLFbQ                Pho Viet Nam #2  33.434528   \n",
       "44   9_HgBRUXUg_vKjTT-GSulA            Golden Hawaiian BBQ  33.393803   \n",
       "143  VxgjMl8tQrT4idJW_z8daA       Carrabba's Italian Grill  33.384757   \n",
       "64   DgZ-pZUo3drzpiCDlDr9IQ                  unPhogettable  33.412909   \n",
       "\n",
       "      longitude                        address  stars  \\\n",
       "89  -111.716897                5251 E Brown Rd    4.5   \n",
       "225 -111.757781             3406 E Baseline Rd    3.0   \n",
       "128 -111.685216       5221 S Power Rd, Are 104    4.0   \n",
       "160 -111.682591                2831 N Power Rd    3.5   \n",
       "105 -111.859793  2950 S Alma School Rd, Ste 12    4.5   \n",
       "25  -111.655685            8253 E Guadalupe Rd    3.5   \n",
       "86  -111.719609      1042 N Higley Rd, Ste 103    4.0   \n",
       "44  -111.868805    1720 W Southern Ave, Ste C3    4.0   \n",
       "143 -111.679534           1740 S Clearview Ave    4.0   \n",
       "64  -111.875826        66 S Dobson Rd, Ste 138    4.0   \n",
       "\n",
       "                                                  text  \\\n",
       "89   I like Italian food. It's probably the only fo...   \n",
       "225  We moved to AZ a few years ago and don't know ...   \n",
       "128  I came here for lunch one day, and tried the P...   \n",
       "160  We went last Saturday night and it was awesome...   \n",
       "105  So delicious! My family has been coming here f...   \n",
       "25   Been here at least a dozen times, while the se...   \n",
       "86   Very nice restaurant, service is always great!...   \n",
       "44   I had always heard great things and finally de...   \n",
       "143  Yay for Carrabbas...after a bad experience wit...   \n",
       "64   I ordered a rare beef pho with added beef ball...   \n",
       "\n",
       "                                             tokenized  \n",
       "89   [like, italian, food, food, take, home, left, ...  \n",
       "225  [move, year, know, mani, seafood, restaur, pre...  \n",
       "128  [lunch, day, pho, bun, bo, hue, substitut, egg...  \n",
       "160  [last, saturday, night, awesom, place, live, m...  \n",
       "105  [delici, famili, come, year, husband, favorit,...  \n",
       "25   [least, dozen, time, servic, poor, food, good,...  \n",
       "86   [nice, restaur, servic, great, choic, asian, c...  \n",
       "44   [great, thing, tri, chicken, katsu, amaz, gold...  \n",
       "143  [bad, experi, tri, get, outback, next, door, c...  \n",
       "64   [order, rare, beef, pho, beef, ball, beef, che...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_name = 'mesa_5000.csv'\n",
    "query = pd.read_csv(csv_name, index_col=0)\n",
    "query = query.loc[query['name']==\"Alessia's Ristorante Italiano\"]\n",
    "query['tokenized'] = query['tokenized'].apply(eval)\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "lsi_model = train_hdp(corpus=corpus, id2word=dictionary, T=12)\n",
    "lsi_model[corpus][0]\n",
    "query_bow = lsi_model.id2word.doc2bow(query.iloc[0]['tokenized'])\n",
    "\n",
    "most_sim_ids = get_most_similar_businesses(query.iloc[0]['tokenized'], corpus, dictionary, lsi_model)\n",
    "tmp = df.iloc[most_sim_ids,:]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LDA\n",
    "def select_num_topics_LDA(dictionary, corpus, texts, end, label, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lda(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "    \n",
    "    plt.plot(range(start, end + 1, step), coherence_values)\n",
    "    plt.title('LDA coherence values on different number of topics')\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lda' + label + '.png')\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_num_topics_HDP(dictionary, corpus, texts, end, label, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the HDP model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, end + 1, step): \n",
    "        model = train_hdp(corpus=corpus, id2word=dictionary, T=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "            \n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "    \n",
    "    plt.plot(range(start, end + 1, step), coherence_values)\n",
    "    plt.title('HDP coherence values on different number of topics')\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('hdp' + label + '.png')\n",
    "\n",
    "    return model, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LSI\n",
    "def select_num_topics_LSI(dictionary, corpus, texts, end, label, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LSI topic models\n",
    "    coherence_values : Coherence values corresponding to the Lsi model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "    \n",
    "    plt.plot(range(start, end + 1, step), coherence_values)\n",
    "    plt.title('LSI coherence values on different number of topics')\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi' + label + '.png')\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameter top_k_words parameter num_topics\n",
    "def eval_top_k_words():\n",
    "    for fn in ('mesa_5000.csv', 'mesa_7500.csv', 'mesa_10000.csv'):\n",
    "        df = pd.read_csv(fn, index_col=0)\n",
    "        df['tokenized'] = df['tokenized'].apply(eval)\n",
    "        dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "        print('preprocessing ' + fn + ' finished')\n",
    "        select_num_topics_LDA(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=30, label=fn)\n",
    "        select_num_topics_HDP(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=30, label=fn)\n",
    "        select_num_topics_LSI(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=30, label=fn)\n",
    "        print('computing ' + fn + ' finished')\n",
    "        \n",
    "eval_top_k_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_name = 'mesa_5000'\n",
    "# df = pd.read_csv(csv_name, index_col=0)\n",
    "# df['tokenized'] = df['tokenized'].apply(eval)\n",
    "# dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "# Set onepass=False, tune power_iters parameter for LSI model\n",
    "def tune_power_iters(dictionary, corpus, num_topics, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_iter in range(3, 31, 3):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    plt.plot(range(3, 31, 3), coherence_values)\n",
    "    plt.title('LSI power_iters parameter experiment')\n",
    "    plt.xlabel(\"Num of iterations\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi_num_iters.png')\n",
    "        \n",
    "    return model_list, coherence_values\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "tune_power_iters(dictionary, corpus, num_topics=12, texts=df['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set onepass=False, tune extra_samples parameter for LSI model\n",
    "def tune_extra_samples(dictionary, corpus, num_topics, num_iter, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_sample in range(100, 300, 30):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter, extra_samples=num_sample)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    plt.plot(range(100, 300, 30), coherence_values)\n",
    "    plt.title('LSI extra_samples parameter experiment')\n",
    "    plt.xlabel(\"Num of extra samples\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi_extra_samples.png')\n",
    "    \n",
    "    return model_list, coherence_values\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "tune_extra_samples(dictionary, corpus, num_topics=12, num_iter=50, texts=df['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "passes = list(range(10, 51, 10))\n",
    "iterations = list(range(100, 410, 60))\n",
    "\n",
    "# Set eval_every to 1 and do grid search of passes and iterations for LDA model\n",
    "def LDA_grid_search(corpus, dictionary, texts, num_topics):\n",
    "    coherence_values = []\n",
    "    pass_iter_pairs = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_pass in passes:\n",
    "        for num_iter in iterations:\n",
    "            pass_iter_pairs.append((num_pass, num_iter))\n",
    "            print(f\"--------------- pass: {num_pass}, iter: {num_iter}--------------\")\n",
    "            model = train_lda(corpus=corpus, id2word=dictionary.id2token, num_topics=num_topics, passes=num_pass,\n",
    "                              iterations=num_iter, eval_every=1, random_state=0)\n",
    "            model_list.append(model)\n",
    "            coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherence_model.get_coherence())\n",
    "            print()\n",
    "            \n",
    "    plt.plot(range(0, len(pass_iter_pairs)), coherence_values)\n",
    "    plt.title('passes and iterations to select best LDA model')\n",
    "    plt.xlabel(\"Index of pass and iteration pairs\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('passes_iters_LDA.png')\n",
    "    \n",
    "    return pass_iter_pairs, model_list, coherence_values\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "pi_pairs, models, coherence_values = LDA_grid_search(corpus=corpus, dictionary=dictionary, texts, num_topics=12, \n",
    "                                                     texts=df['tokenized'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune no_above and no_below parameters in filter_extremes method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search on T, K parameters in HDP model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try two optimized versions of LDA model: ldavowpalwabbit and ldamallet if time allows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Hellinger, Kullback–Leibler to measure distance metrics for probability distributions after selecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.016919672),\n",
       " (2, 0.086762264),\n",
       " (3, 0.032220233),\n",
       " (4, 0.23659779),\n",
       " (5, 0.022509795),\n",
       " (8, 0.01265313),\n",
       " (9, 0.030311227),\n",
       " (10, 0.17654638),\n",
       " (11, 0.37557247)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.025580758),\n",
       " (2, 0.09432872),\n",
       " (3, 0.04854446),\n",
       " (4, 0.20399308),\n",
       " (5, 0.02056593),\n",
       " (8, 0.026410503),\n",
       " (9, 0.036607433),\n",
       " (10, 0.198711),\n",
       " (11, 0.33592972)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhetpyaHWLUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LSI model on businesses:  0.001632988452911377 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   7.95121635,  101.92274244,   59.16168337, -199.53637389,\n",
       "         -7.92914503,    5.96253872])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train HDP model on businesses:  0.007365183035532633 min\n",
      "model[corpus] shape:  284 2\n",
      "doc dist shape:  (284,)\n",
      "model[corpus] shape:  5 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-595847a9e451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhdp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmost_sim_ids_hdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdp_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_most_similar_businesses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m                                                                \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmost_similar_df_hdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmost_sim_ids_hdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36mget_most_similar_businesses\u001b[0;34m(query_data, corpus, dictionary, model)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mdoc_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doc dist shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mnew_doc_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new doct dist shape before flatten: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_doc_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mnew_doc_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_doc_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36mget_topic_dist\u001b[0;34m(model, corpus)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model[corpus] shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model[corpus] shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model[corpus] shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "csv_name = 'mesa_5000.csv'\n",
    "query = pd.read_csv(csv_name, index_col=0)\n",
    "query = query.loc[query['name']==\"Alessia's Ristorante Italiano\"]\n",
    "query['tokenized'] = query['tokenized'].apply(eval)\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "hdp_model = train_hdp(corpus=corpus, id2word=dictionary, T=12, random_state=0)\n",
    "most_sim_ids_hdp, _, hdp_topic_dist = get_most_similar_businesses(query.iloc[0]['tokenized'], corpus,                                                                dictionary, hdp_model)\n",
    "most_similar_df_hdp = df[df.index.isin(most_sim_ids_hdp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(lsi_topic_dist, open(\"processor.pkl\",\"wb\"))\n",
    "preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "# print the results\n",
    "most_similar_df = df[df.index.isin(most_sim_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zONtAIhpfIV_"
   },
   "outputs": [],
   "source": [
    "# get the ids of the most similar businesses\n",
    "# new_bow = dictionary.doc2bow()\n",
    "# new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "# most_sim_ids = get_most_similar_documents(new_doc_distribution, doc_topic_dist)\n",
    "\n",
    "\n",
    "\n",
    "# print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDBy4J73vH3B"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(doc_topic_dist,open(\"processor.pkl\",\"wb\"))\n",
    "preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "# print the results\n",
    "most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OTkrbs7zvoo"
   },
   "outputs": [],
   "source": [
    "# def process_query(preprocessed, query):\n",
    "#   # SQL to pandas DataFrame w/ query\n",
    "#   query = query.groupby(['name'])['text'].apply(' '.join).reset_index()\n",
    "#   query = query[query['text'].map(type) == str]\n",
    "#   query.dropna(axis=0, inplace=True, subset=['text'])\n",
    "#   query['tokenized'] = query['text'].apply(apply_all)\n",
    "  \n",
    "#   # read the cached pickle\n",
    "#   preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "  \n",
    "#   # get the ids of the most similar businesses\n",
    "#   new_bow = dictionary.doc2bow(query.iloc[0,2])\n",
    "#   new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "#   most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "#   # print the results\n",
    "#   most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "#   print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a24ec2668>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGLtJREFUeJzt3XuQXGV+3vHvMxdpJI1uaEYSICHJQkJiN3tDIdhktcRZb2BTBUllsaG8FTuhjJMytlPruEIuhTekkrLXdlJJBa8t21v2uuIlGDtrVUoJcSXYwvFCELvAIrqFtEKgQd3S6Eb36DLSTP/yR/cR7WFG0xp1z+k+/Xyqpqb7zNt9fkdHevTO2+95jyICMzPLlp60CzAzs+ZzuJuZZZDD3cwsgxzuZmYZ5HA3M8sgh7uZWQalGu6SvibphKQ3Gmi7U9K3JU1I+kLd9g2SXpH0qqT9kv5Ra6s2M2t/SnOeu6SdwBjw9Yj46CxtNwLLgH8K7I6IZ2vbF1A9jnFJg8AbwA9ExLFW1m5m1s5S7blHxF7gdP02SZsl/c9ab/wFSdtqbY9ExOtAZcp7XIqI8drThXioycysLYNwF/DTEXEH1V76r832AknrJb0OHAV+yb12M+t2fWkXUK82rPIDwB9ISjYvnO11EXEU+Jikm4BvSno2Io63rlIzs/bWVuFO9TeJsxHxibm8OCKOSdoPfBp4tqmVmZl1kLYalomIEvC2pAcBVPXxq71G0jpJi2qPVwJ3AwdaXqyZWRtLeyrkN4BvAbdJGpH0CPCjwCOSXgP2Aw/U2v5VSSPAg8Bv1HroANuBl2rt/wz4lYj47nwfi5lZO0l1KqSZmbVGWw3LmJlZc6T2gerQ0FBs3Lgxrd2bmXWkV1555WREDM/WLrVw37hxI/v27Utr92ZmHUnSO42087CMmVkGOdzNzDLI4W5mlkEOdzOzDHK4m5llkMPdzCyDHO5mZhnkcDczmyeTleDf7cnx2tGzLd+Xw93MbJ68ffIcu/Ye5uCJsZbvy+FuZjZP8sUSANvWLm35vhzuZmbzJF8o09sjbl092PJ9OdzNzOZJvlhi8/ASBvp7W74vh7uZ2TzJFcpsW7tsXvblcDczmweli5d57+wFtt3Y+vF2cLibmc2LA8UyANvdczczy458oTZTxj13M7PsyBXLLF/Uz9plA/OyP4e7mdk8yBdKbFu7FEnzsr9Zw13S1ySdkPTGDD+XpP8k6ZCk1yV9qvllmpl1rkolOFAss/3G+Rlvh8Z67r8D3HuVn98HbKl9PQp89frLMjPLjqNnznPu0uS8XJmamDXcI2IvcPoqTR4Avh5VLwIrJN3YrALNzDpdrlCdKbOtzXrus7kZOFr3fKS27UMkPSppn6R9o6OjTdi1mVn7yxdLSLB1TeuXHUg0I9yn+3QgpmsYEbsiYkdE7BgeHm7Crs3M2l++UGbjqiUsXtA3b/tsRriPAOvrnq8DjjXhfc3MMiFfLLF9nua3J5oR7ruBv1+bNXMX8H5EFJrwvmZmHe/c+ATvnD4/b2vKJGb9HUHSN4B7gCFJI8AvAP0AEfHrwB7g88Ah4DzwD1pVrJlZp3nreJmI+VnDvd6s4R4RD8/y8wB+qmkVmZllSD5ZU2YeZ8qAr1A1M2upfKHE4MI+bl6xaF7363A3M2uhXLHMbWuX0tMzP8sOJBzuZmYtEhFX1pSZbw53M7MWOfb+RUoXJ+b1ytSEw93MrEWSNdy3u+duZpYdyUyZrQ53M7PsyBVKrFu5iGUD/fO+b4e7mVmL5Od5Dfd6Dnczsxa4eHmSw6NjqYy3g8PdzKwlDp0YoxLzu4Z7PYe7mVkL5GozZdKY4w4OdzOzlsgXywz097Bh1ZJU9u9wNzNrgXyxxG1rltI7z8sOJBzuZmZNFhHkCuV5X8O9nsPdzKzJRsvjnD53iW3zfPeleg53M7Mmy9WuTHXP3cwsQ/Ipz5QBh7uZWdPli2XWLhtg5ZIFqdXgcDcza7JcocT2FMfbweFuZtZUlyYqfG90LLUrUxMOdzOzJjp8cozLk5HqeDs43M3MmipfqM6USWs1yITD3cysiXLFEgt6e9g0lM6yAwmHu5lZE+ULZW5dPUh/b7rx6nA3M2uifLGU6pWpCYe7mVmTnD53ieOlcbaneGVqwuFuZtYkV65Mdc/dzCw72mFNmURD4S7pXkkHJB2S9Pg0P79F0vOSviPpdUmfb36pZmbtLV8oMTS4gOGlC9MuZfZwl9QLPAXcB9wOPCzp9inN/hXwTER8EngI+LVmF2pm1u7yxXLq89sTjfTc7wQORcThiLgEPA08MKVNAMkRLQeONa9EM7P2NzFZ4a3j5dSvTE00Eu43A0frno/UttX7MvBFSSPAHuCnp3sjSY9K2idp3+jo6BzKNTNrT0dOnWd8otIW4+3QWLhPdwPAmPL8YeB3ImId8Hng9yR96L0jYldE7IiIHcPDw9derZlZm8oX22emDDQW7iPA+rrn6/jwsMsjwDMAEfEtYAAYakaBZmadIF8o09sjbl09mHYpQGPh/jKwRdImSQuofmC6e0qbd4G/CSBpO9Vw97iLmXWNfLHE5uElLOzrTbsUoIFwj4gJ4DHgOSBHdVbMfklPSrq/1uzngJ+Q9BrwDeDHI2Lq0I2ZWWblCuW2GW8H6GukUUTsofpBaf22J+oevwnc3dzSzMw6w/sXLvPe2Qv86F23pF3KFb5C1czsOh2oXZnaDmvKJBzuZmbXqd1myoDD3czsuuUKZZYv6mftsoG0S7nC4W5mdp3yxRLb1i5Fmu6yoHQ43M3MrkOlEhxoozVlEg53M7PrcPTMec5fmmR7G423g8PdzOy65Arts4Z7PYe7mdl1yBdLSLB1jXvuZmaZkS+U2bRqCYsWtMeyAwmHu5nZdcgVS201vz3hcDczm6Nz4xO8c+p82423g8PdzGzODhxPPkx1z93MLDPytZky7TbHHRzuZmZzli+WGFzYx80rFqVdyoc43M3M5ihfKHPb2qX09LTPsgMJh7uZ2RxEBLliqe2uTE043M3M5uDY+xcpX5xoy5ky4HA3M5uTfKG6hrt77mZmGZKv3X2p3ZYdSDjczczmIFcosf6GRSwd6E+7lGk53M3M5iBXKLXteDs43M3MrtnFy5O8ffIc29vwytSEw93M7BodPD5GJWBbG16ZmnC4m5ldo1yxOlOmHdeUSTjczcyuUb5QZqC/hw2rlqRdyowc7mZm1yhfLHHbmqX0tuGyAwmHu5nZNYgIcoVSW64EWc/hbmZ2DUbL45w5f7mtx9uhwXCXdK+kA5IOSXp8hjY/LOlNSfsl/X5zyzQzaw+52pWp7TxTBqBvtgaSeoGngB8CRoCXJe2OiDfr2mwB/jlwd0SckbS6VQWbmaUpWVMmCz33O4FDEXE4Ii4BTwMPTGnzE8BTEXEGICJONLdMM7P2kC+WuXH5ACsWL0i7lKtqJNxvBo7WPR+pbau3Fdgq6f9KelHSvdO9kaRHJe2TtG90dHRuFZuZpai67EB799qhsXCfbq5PTHneB2wB7gEeBn5L0ooPvShiV0TsiIgdw8PD11qrmVmqLk1UOHRirO3H26GxcB8B1tc9Xwccm6bNH0fE5Yh4GzhANezNzDLje6NjTFQiMz33l4EtkjZJWgA8BOye0uabwN8AkDREdZjmcDMLNTNLW76Y3KAjAz33iJgAHgOeA3LAMxGxX9KTku6vNXsOOCXpTeB54Ocj4lSrijYzS0O+UGZBbw+bhtp32YHErFMhASJiD7BnyrYn6h4H8KXal5lZJuWKZW5dPUh/b/tf/9n+FZqZtYl8Byw7kHC4m5k14NTYOCfK4217Q+ypHO5mZg04kCw70Ma31qvncDcza8AHa8q4525mlhn5QomhwYUMDS5Mu5SGONzNzBqQK5Y6ZrwdHO5mZrOamKzw1vGxjrgyNeFwNzObxZFT57g0UemYD1PB4W5mNqtcobM+TAWHu5nZrPLFEr094tbVg2mX0jCHu5nZLPKFMpuHl7CwrzftUhrmcDczm0W+WO6o8XZwuJuZXdX7Fy7z3tkLHbOmTMLhbmZ2FQc67MrUhMPdzOwqrtygw8MyZmbZkSuUWbG4nzXLOmPZgYTD3czsKnKFEtvWLkVS2qVcE4e7mdkMKpXgQAfOlAGHu5nZjN49fZ4Llyc7asGwhMPdzGwGyYep7rmbmWVIrlBGgq1r3HM3M8uMfLHEplVLWLSgc5YdSDjczcxmkC+WO+7ipYTD3cxsGufGJ3jn1PmOu3gp4XA3M5vGgePJsgMOdzOzzMgnN+jooFvr1XO4m5lNI18sMbiwj3UrF6Vdypw43M3MptGpyw4kGgp3SfdKOiDpkKTHr9LuC5JC0o7mlWhmNr8ignyhc2fKQAPhLqkXeAq4D7gdeFjS7dO0Wwr8DPBSs4s0M5tP7529QHl8oiOvTE000nO/EzgUEYcj4hLwNPDANO3+DfAV4GIT6zMzm3fJh6mduKZMopFwvxk4Wvd8pLbtCkmfBNZHxH+/2htJelTSPkn7RkdHr7lYM7P5kKwp04nLDiQaCffpPk2IKz+UeoD/APzcbG8UEbsiYkdE7BgeHm68SjOzeZQrlll/wyKWDvSnXcqcNRLuI8D6uufrgGN1z5cCHwX+VNIR4C5gtz9UNbNOlS+UOnq8HRoL95eBLZI2SVoAPATsTn4YEe9HxFBEbIyIjcCLwP0Rsa8lFZuZtdDFy5O8ffIc2zv0ytTErOEeERPAY8BzQA54JiL2S3pS0v2tLtDMbD4dPD5GJWB7h16ZmuhrpFFE7AH2TNn2xAxt77n+sszM0pFLbtCR9Z67mVk3yRfKLOrv5ZYbFqddynVxuJuZ1ckVSmxdu5Tens5cdiDhcDczq4kI8sVSx4+3g8PdzOyKE+Vxzpy/3LHL/NZzuJuZ1eQK2fgwFRzuZmZX5IudfYOOeg53M7OafKHEjcsHWLF4QdqlXDeHu5lZTb5YzkSvHRzuZmYAXJqocOjEWMcvO5BwuJuZAd8bHWOiEpn4MBUc7mZmwAdruGdhjjs43M3MgOqyAwt6e9g0tCTtUprC4W5mBrxZKLFlzSB9vdmIxWwchZnZdarOlMnGeDs43M3MODk2zmh5vKNviD2Vw93Mut6BK1emuuduZpYZH6wp4567mVlm5ItlhgYXMjS4MO1SmsbhbmZdL18sZWq8HRzuZtblJiYrvHV8LDNryiQc7mbW1Y6cOseliUpm1pRJONzNrKvlCtmbKQMOdzPrcvliib4esXl1NpYdSDjczayr5QplNg8PsrCvN+1SmsrhbmZdLV8oZWp+e8LhbmZd6/3zlzn2/sXMjbeDw93Muliyhrt77mZmGZKvrSmzvVt77pLulXRA0iFJj0/z8y9JelPS65L+t6QNzS/VzKy58sUSKxb3s2ZZdpYdSMwa7pJ6gaeA+4DbgYcl3T6l2XeAHRHxMeBZ4CvNLtTMrNlyhTLb1i5FUtqlNF0jPfc7gUMRcTgiLgFPAw/UN4iI5yPifO3pi8C65pZpZtZclUpwIGM36KjXSLjfDBytez5S2zaTR4D/Md0PJD0qaZ+kfaOjo41XaWbWZO+ePs+Fy5PcnrFlBxKNhPt0v6/EtA2lLwI7gF+e7ucRsSsidkTEjuHh4carNDNrsizPlAHoa6DNCLC+7vk64NjURpI+C/xL4DMRMd6c8szMWiNXKNMj2LI6m+HeSM/9ZWCLpE2SFgAPAbvrG0j6JPAbwP0RcaL5ZZqZNVeuUGLj0BIWLcjWsgOJWcM9IiaAx4DngBzwTETsl/SkpPtrzX4ZGAT+QNKrknbP8HZmZm0hXyxncn57opFhGSJiD7BnyrYn6h5/tsl1mZm1zNj4BO+ePs+Dd2R3Yp+vUDWzrnOgdmXqtozOlAGHu5l1oSszZTJ2a716Dncz6zr5QpnBhX2sW7ko7VJaxuFuZl0nXyxldtmBhMPdzLpKRJAvlDN78VLC4W5mXeW9sxcoj0+wPcMfpoLD3cy6TL5QmymT4Tnu4HA3sy6TzJS5LcMzZcDhbmZdJlcoc8sNixlc2NA1nB3L4W5mXSVXmymTdQ53M+saFy5NcuTkuUxfmZpwuJtZ1zh4okwlYLt77mZm2XFlpox77mZm2ZErlljU38stNyxOu5SWc7ibWdfIF8psXbuU3p7sLjuQcLibWVeICPLFUleMt4PD3cy6xInyOGfOX878sgMJh7uZdYVcIftruNdzuJtZV8gXu2NNmYTD3cy6Qq5Q4qblAyxf3J92KfMi24srmFnXO3PuEn9+6CQvHT7N7Td1R68dHO5mljETkxVePXqWvW+N8mcHT/L6yFkiYNlAH/d//Ka0y5s3Dncz63hHT59n78FR9r41yl8cOkV5fIIewSfWr+BnfnALO7cO8/F1y+nr7Z6RaIe7mXWcc+MTvHj4FC8cPMnet0Y5fPIcADctH+Bvf+xGdm4d5u7NQ10zvj4dh7uZtb1KJcgVS+x9qxrm+945zeXJYKC/h7u+bxVfvGsDO7cOs3l4SaZven0tHO5m1pZOjo3zwsFR9r51khcOnuTk2DhQnaf+D+/exM6tw9yxYSUD/b0pV9qeHO5m1hYuTVTY987pWpiPsv9Y9aKjG5Ys4NNbhti5ZZhPbxli9bKBlCvtDA53M0tFRHDk1Hn2vlX9IPRbh09x/tIkfT3ijg0r+fm/dRs7twzzkZuW0dMFC301W0PhLule4D8CvcBvRcQvTvn5QuDrwB3AKeBHIuJIc0s1s05XuniZvzh06srMlpEzFwDYsGoxf+9T69i5dZjv37wq8/c3nQ+z/glK6gWeAn4IGAFelrQ7It6sa/YIcCYibpX0EPBLwI+0omAza76IYLISTFSCStS+V/7y98nkK+oeV6a8bvLDr5+sBAePl9l7cJRvv3uWyUowuLCP79+8ip/8zGZ2bhliw6olaf8RZE4j/z3eCRyKiMMAkp4GHgDqw/0B4Mu1x88C/1mSIiKaWCsAz7x8lN984XCz39Yyoul/4eZopr/6026doejpNl/T+wKVCCoVmKhUmKzAZKUybUhXWvwHJ8FfuXk5//gzm/n0liE+tWEl/V005zwNjYT7zcDRuucjwF+bqU1ETEh6H1gFnKxvJOlR4FGAW265ZU4Fr1jcz5Y1g3N6rXUH0SbjszOUMd3mmabvTd/22t63t0f0SvT21r73VL/6ekRP8l3V77O26U3a9tDbA7313+te95deL7F2+QA3LFkwfeHWEo2E+3R/Z6b+P99IGyJiF7ALYMeOHXPqK3zuI2v53EfWzuWlZmZdo5Hfi0aA9XXP1wHHZmojqQ9YDpxuRoFmZnbtGgn3l4EtkjZJWgA8BOye0mY38GO1x18A/k8rxtvNzKwxsw7L1MbQHwOeozoV8msRsV/Sk8C+iNgN/Dbwe5IOUe2xP9TKos3M7OoamkwaEXuAPVO2PVH3+CLwYHNLMzOzufJcJDOzDHK4m5llkMPdzCyDHO5mZhmktGYsShoF3kll51c3xJQrazMm68cH2T9GH1/nu55j3BARw7M1Si3c25WkfRGxI+06WiXrxwfZP0YfX+ebj2P0sIyZWQY53M3MMsjh/mG70i6gxbJ+fJD9Y/Txdb6WH6PH3M3MMsg9dzOzDHK4m5llkMO9RtIRSd+V9KqkfWnX0wySvibphKQ36rbdIOlPJB2sfV+ZZo3XY4bj+7Kk92rn8VVJn0+zxushab2k5yXlJO2X9LO17Vk6hzMdYybOo6QBSf9P0mu14/vXte2bJL1UO4f/tbacenP37TH3KklHgB0RkZmLJyTtBMaAr0fER2vbvgKcjohflPQ4sDIi/lmadc7VDMf3ZWAsIn4lzdqaQdKNwI0R8W1JS4FXgL8D/DjZOYczHeMPk4HzqOr9E5dExJikfuDPgZ8FvgT8UUQ8LenXgdci4qvN3Ld77hkWEXv58B2xHgB+t/b4d6n+Q+pIMxxfZkREISK+XXtcBnJU71ecpXM40zFmQlSN1Z72174C+EHg2dr2lpxDh/sHAvhfkl6p3cg7q9ZERAGq/7CA1SnX0wqPSXq9NmzTsUMW9SRtBD4JvERGz+GUY4SMnEdJvZJeBU4AfwJ8DzgbERO1JiO04D80h/sH7o6ITwH3AT9V+5XfOs9Xgc3AJ4AC8KvplnP9JA0Cfwj8k4gopV1PK0xzjJk5jxExGRGfoHr/6TuB7dM1a/Z+He41EXGs9v0E8N+onoQsOl4b50zGO0+kXE9TRcTx2j+mCvCbdPh5rI3T/iHwXyLij2qbM3UOpzvGrJ1HgIg4C/wpcBewQlJyJ7x1wLFm78/hDkhaUvswB0lLgM8Bb1z9VR2r/mbmPwb8cYq1NF0SejV/lw4+j7UP434byEXEv6/7UWbO4UzHmJXzKGlY0ora40XAZ6l+rvA88IVas5acQ8+WASR9H9XeOlTvK/v7EfFvUyypKSR9A7iH6vKix4FfAL4JPAPcArwLPBgRHfmh5AzHdw/VX+UDOAL8ZDI+3Wkk/XXgBeC7QKW2+V9QHZPOyjmc6RgfJgPnUdLHqH5g2ku1M/1MRDxZy5yngRuA7wBfjIjxpu7b4W5mlj0eljEzyyCHu5lZBjnczcwyyOFuZpZBDnczswxyuJuZZZDD3cwsg/4/bD8W3GRnoRgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.arange(3, 31, 3)\n",
    "y = np.exp(x)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import is_corpus\n",
    "corpus = [(1, 1.0)]\n",
    "corpus_or_not, corpus = is_corpus(corpus)\n",
    "corpus_or_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FunctionLDA.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
