{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7TpVKeITKFY"
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print('Downloads Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbTRSwxgTvUC"
   },
   "outputs": [],
   "source": [
    "def initial_clean(text):\n",
    "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = text.lower() # lower case the text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "def pos(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "informative_pos = ('JJ','VB', 'NN','RBS','VBP','IN','RBR','JJR','JJS','PDT','RP','UH','FW','NNS','VBN','VBG')\n",
    "def remove_uninformative_pos(text):\n",
    "    tagged_words = nltk.pos_tag(text)\n",
    "    return [word for word, tag in tagged_words if tag in informative_pos]\n",
    "  \n",
    "clutter = ['food','place','good','order','great','like',\n",
    "           'service','time','go','ordered','get','love',\n",
    "           'best','come','eat','dont','tried','try','ask',\n",
    "           'nice','restaurant','ive','im','didnt']\n",
    "def remove_garbage(text):\n",
    "    return [word for word in text if word not in clutter]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
    "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(remove_garbage(remove_uninformative_pos(remove_stop_words(initial_clean(text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbaeAOu4XmKV"
   },
   "outputs": [],
   "source": [
    "csv_name = 'csv_reviews_mesa.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdR22g-xTOCV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean and tokenize 284 businesses' reviews: 3.9660783489545186 min\n"
     ]
    }
   ],
   "source": [
    "# format the columns\n",
    "df = pd.read_csv(csv_name)\n",
    "df = df.groupby(['business_id', 'name'])['text'].apply(' '.join).reset_index()\n",
    "df = df[df['text'].map(type) == str]\n",
    "df.dropna(axis=0, inplace=True, subset=['text'])\n",
    "\n",
    "# preprocess the text and business name and create new column \"tokenized\"\n",
    "t1 = time.time()\n",
    "df['tokenized'] = df['text'].apply(apply_all)\n",
    "t2 = time.time()\n",
    "print(\"Time to clean and tokenize\", len(df), \"businesses' reviews:\", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_EiI6qhWCC7S"
   },
   "outputs": [],
   "source": [
    "num_topics = 12\n",
    "k = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vXAL6BaNTf9Y"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3oxnPPPU3YoxO9M1I2idg</td>\n",
       "      <td>Eklectic Pie - Mesa</td>\n",
       "      <td>Tried this place for the first time, the pizza...</td>\n",
       "      <td>[first, pizza, kept, take, pizza, cut, pizza, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-oSII3bw90cvyLmgsHgmpg</td>\n",
       "      <td>Mokis Hawaiian Grill</td>\n",
       "      <td>First time going and We loved the teriyaki chi...</td>\n",
       "      <td>[first, go, teriyaki, chicken, teriyaki, beef,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-sNi7U9seVfCr8T8nkWd_w</td>\n",
       "      <td>Rumbi Island Grill</td>\n",
       "      <td>Absolutely delicious. The balsamic dressing I ...</td>\n",
       "      <td>[delici, balsam, dress, drink, straw, chicken,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01xTdrNUuTOAyH7NaRWcUA</td>\n",
       "      <td>Mellow Mushroom</td>\n",
       "      <td>Hi went here yesterday with my sister who has ...</td>\n",
       "      <td>[hi, yesterday, sister, vegan, year, onlin, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09psTuUYhUMA2ZRzQlm30Q</td>\n",
       "      <td>Five Guys</td>\n",
       "      <td>Five Guys Burger and Fries... you have alot of...</td>\n",
       "      <td>[guy, burger, fri, alot, explain, ice, tea, fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                  name  \\\n",
       "0  -3oxnPPPU3YoxO9M1I2idg   Eklectic Pie - Mesa   \n",
       "1  -oSII3bw90cvyLmgsHgmpg  Mokis Hawaiian Grill   \n",
       "2  -sNi7U9seVfCr8T8nkWd_w    Rumbi Island Grill   \n",
       "3  01xTdrNUuTOAyH7NaRWcUA       Mellow Mushroom   \n",
       "4  09psTuUYhUMA2ZRzQlm30Q             Five Guys   \n",
       "\n",
       "                                                text  \\\n",
       "0  Tried this place for the first time, the pizza...   \n",
       "1  First time going and We loved the teriyaki chi...   \n",
       "2  Absolutely delicious. The balsamic dressing I ...   \n",
       "3  Hi went here yesterday with my sister who has ...   \n",
       "4  Five Guys Burger and Fries... you have alot of...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [first, pizza, kept, take, pizza, cut, pizza, ...  \n",
       "1  [first, go, teriyaki, chicken, teriyaki, beef,...  \n",
       "2  [delici, balsam, dress, drink, straw, chicken,...  \n",
       "3  [hi, yesterday, sister, vegan, year, onlin, ex...  \n",
       "4  [guy, burger, fri, alot, explain, ice, tea, fr...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first get a list of all words\n",
    "all_words = [word for item in list(df['tokenized']) for word in item]\n",
    "\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "# define a function only to keep words in the top k words\n",
    "top_k_words,_ = zip(*fdist.most_common(k))\n",
    "\n",
    "top_k_words = set(top_k_words)\n",
    "def keep_top_k_words(text):\n",
    "    return [word for word in text if word in top_k_words]\n",
    "df['tokenized'] = df['tokenized'].apply(keep_top_k_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOMcVS6WTeJz"
   },
   "outputs": [],
   "source": [
    "def train_hdp(data):\n",
    "    \"\"\"\n",
    "    This function trains the hdp model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary(data['tokenized'])\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    hdp = HdpModel(corpus=corpus, id2word=dictionary)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train HDP model on \", len(df), \"businesses: \", (t2-t1)/60, \"min\")\n",
    "    return dictionary,corpus,hdp\n",
    "  \n",
    "# train the topic model\n",
    "dictionary,corpus,hdp = train_hdp(df)\n",
    "\n",
    "# get the topic distribution\n",
    "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in hdp[corpus]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wpMk8BETxnA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LDA model on  284 businesses:  0.04426158269246419 min\n",
      "(10, '0.007*\"delici\" + 0.006*\"chicken\" + 0.005*\"price\" + 0.005*\"sauc\" + 0.005*\"wait\" + 0.005*\"menu\" + 0.005*\"staff\" + 0.005*\"tabl\" + 0.004*\"want\" + 0.004*\"fresh\"')\n",
      "(4, '0.007*\"delici\" + 0.005*\"chicken\" + 0.005*\"amaz\" + 0.005*\"tabl\" + 0.005*\"wait\" + 0.005*\"flavor\" + 0.005*\"price\" + 0.004*\"sandwich\" + 0.004*\"drink\" + 0.004*\"staff\"')\n",
      "(1, '0.007*\"menu\" + 0.006*\"wait\" + 0.006*\"chicken\" + 0.006*\"salad\" + 0.005*\"delici\" + 0.005*\"meal\" + 0.005*\"price\" + 0.005*\"staff\" + 0.005*\"littl\" + 0.004*\"amaz\"')\n",
      "(9, '0.008*\"chicken\" + 0.006*\"pizza\" + 0.005*\"sauc\" + 0.005*\"price\" + 0.005*\"drink\" + 0.005*\"sandwich\" + 0.005*\"littl\" + 0.005*\"menu\" + 0.005*\"fri\" + 0.004*\"delici\"')\n",
      "(6, '0.006*\"chicken\" + 0.006*\"delici\" + 0.006*\"sauc\" + 0.006*\"price\" + 0.006*\"menu\" + 0.005*\"salad\" + 0.005*\"make\" + 0.005*\"staff\" + 0.005*\"wait\" + 0.005*\"littl\"')\n",
      "(2, '0.008*\"chicken\" + 0.006*\"pizza\" + 0.006*\"chees\" + 0.006*\"fri\" + 0.005*\"littl\" + 0.005*\"delici\" + 0.005*\"price\" + 0.005*\"menu\" + 0.005*\"amaz\" + 0.004*\"staff\"')\n",
      "(8, '0.007*\"chicken\" + 0.006*\"pizza\" + 0.005*\"sauc\" + 0.005*\"staff\" + 0.005*\"delici\" + 0.005*\"tabl\" + 0.005*\"hot\" + 0.004*\"flavor\" + 0.004*\"wait\" + 0.004*\"meal\"')\n",
      "(3, '0.008*\"chicken\" + 0.006*\"delici\" + 0.006*\"price\" + 0.005*\"wait\" + 0.005*\"sauc\" + 0.005*\"menu\" + 0.005*\"staff\" + 0.005*\"tabl\" + 0.005*\"flavor\" + 0.005*\"tast\"')\n",
      "(5, '0.007*\"chicken\" + 0.006*\"wait\" + 0.006*\"delici\" + 0.005*\"salad\" + 0.005*\"amaz\" + 0.005*\"tabl\" + 0.005*\"littl\" + 0.005*\"tast\" + 0.004*\"flavor\" + 0.004*\"lunch\"')\n",
      "(0, '0.007*\"chicken\" + 0.006*\"sauc\" + 0.006*\"wait\" + 0.005*\"littl\" + 0.005*\"drink\" + 0.005*\"staff\" + 0.005*\"delici\" + 0.005*\"salad\" + 0.004*\"menu\" + 0.004*\"recommend\"')\n"
     ]
    }
   ],
   "source": [
    "def train_lda(data, chunksize):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary(data['tokenized'])\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=1)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on \", len(df), \"businesses: \", (t2-t1)/60, \"min\")\n",
    "    return dictionary,corpus,lda\n",
    "  \n",
    "# train the topic model\n",
    "dictionary,corpus,lda = train_lda(df, 300)\n",
    "topics = lda.show_topics()\n",
    "for t in topics:\n",
    "    print(t)\n",
    "\n",
    "# get the topic distribution\n",
    "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WS1yYiW9Ehc"
   },
   "outputs": [],
   "source": [
    "def train_lsi(data):\n",
    "    \"\"\"\n",
    "    This function trains the lsi model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary(data['tokenized'])\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lsi = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LSI model on \", len(df), \"businesses: \", (t2-t1)/60, \"min\")\n",
    "    return dictionary,corpus,lsi\n",
    "  \n",
    "# train the topic model\n",
    "dictionary,corpus,lsi = train_lsi(df)\n",
    "\n",
    "# get the topic distribution\n",
    "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lsi[corpus]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhetpyaHWLUm"
   },
   "outputs": [],
   "source": [
    "query = pd.read_csv(csv_name)\n",
    "query = query.loc[query['name']==\"Alessia's Ristorante Italiano\"]\n",
    "query = query.groupby(['name'])['text'].apply(' '.join).reset_index()\n",
    "query = query[query['text'].map(type) == str]\n",
    "query.dropna(axis=0, inplace=True, subset=['text'])\n",
    "query['tokenized'] = query['text'].apply(apply_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wP-SK_5PNlID"
   },
   "outputs": [],
   "source": [
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    # lets keep with the p,q notation above\n",
    "    p = query[None,:].T # take transpose\n",
    "    q = matrix.T # transpose matrix\n",
    "    m = 0.5*(p + q)\n",
    "    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))\n",
    "  \n",
    "def get_most_similar_documents(query,matrix,k=10):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    sims = jensen_shannon(query,matrix) # list of jensen shannon distances\n",
    "    return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zONtAIhpfIV_"
   },
   "outputs": [],
   "source": [
    "# get the ids of the most similar businesses\n",
    "new_bow = dictionary.doc2bow(query.iloc[0,2])\n",
    "new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)\n",
    "\n",
    "# print the results\n",
    "most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjGgDOI85H9k"
   },
   "outputs": [],
   "source": [
    "# Compute Coherence Score using c_v\n",
    "coherence_model = CoherenceModel(model=lda, texts=df['tokenized'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDBy4J73vH3B"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle.dump(doc_topic_dist,open(\"processor.pkl\",\"wb\"))\n",
    "# preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "\n",
    "# most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "# # print the results\n",
    "# most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "# print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OTkrbs7zvoo"
   },
   "outputs": [],
   "source": [
    "# def process_query(preprocessed, query):\n",
    "#   # SQL to pandas DataFrame w/ query\n",
    "#   query = query.groupby(['name'])['text'].apply(' '.join).reset_index()\n",
    "#   query = query[query['text'].map(type) == str]\n",
    "#   query.dropna(axis=0, inplace=True, subset=['text'])\n",
    "#   query['tokenized'] = query['text'].apply(apply_all)\n",
    "  \n",
    "#   # read the cached pickle\n",
    "#   preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "  \n",
    "#   # get the ids of the most similar businesses\n",
    "#   new_bow = dictionary.doc2bow(query.iloc[0,2])\n",
    "#   new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "#   most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "#   # print the results\n",
    "#   most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "#   print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edspkMuuWS4c"
   },
   "outputs": [],
   "source": [
    "def evaluate_graph(dictionary, corpus, texts, limit):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    step = 1\n",
    "    for num_topics in range(60, limit, step):\n",
    "        lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        lm_list.append(lm)\n",
    "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "        print('Level {} Complete'.format(num_topics))\n",
    "        \n",
    "    # Show graph\n",
    "    x = range(60, limit, step)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return lm_list, c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pk1K7LXlWxHP"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lmlist, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], limit=70)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FunctionLDA.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
