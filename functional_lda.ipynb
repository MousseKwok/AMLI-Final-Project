{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7TpVKeITKFY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import os\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print('Downloads Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbTRSwxgTvUC"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def initial_clean(text):\n",
    "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = text.lower() # lower case the text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "def pos(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "informative_pos = ('JJ','VB', 'NN','RBS','VBP','IN','RBR','JJR','JJS','PDT','RP','UH','FW','NNS','VBN','VBG')\n",
    "\n",
    "def remove_uninformative_pos(text):\n",
    "    tagged_words = nltk.pos_tag(text)\n",
    "    return [word for word, tag in tagged_words if tag in informative_pos]\n",
    "  \n",
    "clutter = ['food','place','good','order','great','like',\n",
    "           'service','time','go','ordered','get','love',\n",
    "           'best','come','eat','dont','tried','try','ask',\n",
    "           'nice','restaurant','ive','im','didnt']\n",
    "\n",
    "def remove_garbage(text):\n",
    "    return [word for word in text if word not in clutter]\n",
    "\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
    "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(remove_garbage(remove_uninformative_pos(remove_stop_words(initial_clean(text)))))\n",
    "\n",
    "def get_top_k_words(df, k = 10000):\n",
    "    # first get a list of all words\n",
    "    all_words = [word for item in list(df['tokenized']) for word in item]\n",
    "    \n",
    "    # use nltk fdist to get a frequency distribution of all words\n",
    "    fdist = FreqDist(all_words)\n",
    "    \n",
    "    # define a function only to keep words in the top k words\n",
    "    top_k_words, _ = zip(*fdist.most_common(k))\n",
    "    top_k_words = set(top_k_words)\n",
    "    \n",
    "    return top_k_words\n",
    "\n",
    "def keep_top_k_words(text, *top_k_words):\n",
    "    return [word for word in text if word in top_k_words]\n",
    "\n",
    "def transform_dataset(df):\n",
    "    # format the columns\n",
    "    df = df.groupby(['business_id', 'name'])['text'].apply(' '.join).reset_index()\n",
    "    df = df[df['text'].map(type) == str]\n",
    "    df.dropna(axis=0, inplace=True, subset=['text'])\n",
    "    return df\n",
    "\n",
    "def gen_tokenized_column(df):\n",
    "    # preprocess the text and business name and create new column \"tokenized\"\n",
    "    df['tokenized'] = df['text'].apply(apply_all)\n",
    "    top_k_words = get_top_k_words(df)\n",
    "    df['tokenized'] = df['tokenized'].apply(keep_top_k_words, args=(top_k_words))\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(df):\n",
    "    t1 = time.time()\n",
    "    preprocessed_df = gen_tokenized_column(transform_dataset(df))\n",
    "    t2 = time.time()\n",
    "    print(\"Time to clean and tokenize\", len(df), \"businesses' reviews:\", (t2-t1)/60, \"min\")\n",
    "    return preprocessed_df\n",
    "    \n",
    "def get_coherence_score(model, texts, dictionary):\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "def get_dictionary_corpus(data, no_below=5, no_above=0.1):\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def get_perplexity(model, corpus):\n",
    "    # a measure of how good the model is; lower the better\n",
    "    return lda_model.log_perplexity(corpus)\n",
    "\n",
    "def train_lda(corpus, id2word, chunksize=2000, num_topics=12, alpha='auto', eta='auto', passes=1, iterations=50,\n",
    "              minimum_probability=0.01, eval_every=10, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, alpha=alpha, eta=eta, \n",
    "                   chunksize=chunksize, minimum_probability=minimum_probability, passes=passes, \n",
    "                   iterations=iterations, eval_every=eval_every, random_state=random_state)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lda\n",
    "\n",
    "def train_hdp(corpus, id2word, chunksize=2000, T=150, K=15, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the hdp model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    hdp = HdpModel(corpus=corpus, id2word=id2word, T=T, K=K, chunksize=chunksize, random_state=random_state)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train HDP model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return hdp\n",
    "\n",
    "def train_lsi(corpus, id2word, num_topics=12, chunksize=2000, onepass=True, power_iters=2, extra_samples=100):\n",
    "    \"\"\"\n",
    "    This function trains the lsi model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lsi = LsiModel(corpus=corpus, num_topics=num_topics, id2word=id2word, chunksize=chunksize)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LSI model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lsi\n",
    "\n",
    "def get_most_similar_documents(query, corpus, dictionary, k=10):\n",
    "    distances = []\n",
    "    for c in corpus:\n",
    "        distances.append(kullback_leibler(query, c, num_features=len(dictionary)))\n",
    "    \n",
    "    indices = np.array(distances).argsort()[:k]\n",
    "    return indices\n",
    "\n",
    "def get_topic_dist(model, corpus):\n",
    "    doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in model[corpus]])\n",
    "    return doc_topic_dist\n",
    "\n",
    "def get_most_similar_businesses(query_data, corpus, dictionary, model):\n",
    "    query_bow = dictionary.doc2bow(query_data)\n",
    "    most_sim_ids = get_most_similar_documents(model[query_bow], model[corpus], dictionary)\n",
    "    return most_sim_ids \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LDA\n",
    "def select_num_topics_LDA(dictionary, corpus, texts, end, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lda(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_num_topics_HDP(dictionary, corpus, texts, end, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the HDP model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, end + 1, step): \n",
    "        model = train_hdp(corpus=corpus, id2word=dictionary, T=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "            \n",
    "        coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "    \n",
    "    return model, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LSI\n",
    "def select_num_topics_LSI(dictionary, corpus, texts, end, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LSI topic models\n",
    "    coherence_values : Coherence values corresponding to the Lsi model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameter top_k_words parameter num_topics\n",
    "def eval_top_k_words():\n",
    "    df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "    df['tokenized'] = df['tokenized'].apply(eval)\n",
    "    dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "    print('preprocessing finished')\n",
    "    _, lsi_coherence = select_num_topics_LSI(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=30)\n",
    "    _, lda_coherence = select_num_topics_LDA(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=30)\n",
    "    _, hdp_coherence = select_num_topics_HDP(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=30)\n",
    "    x = range(3, 31, 3)\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(x, lda_coherence, color='r', label='lda')\n",
    "    ax.plot(x, hdp_coherence, color='g', label='hdp')\n",
    "    ax.plot(x, lsi_coherence, color='b', label='lsi')\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.set(xlabel='Num of topics', ylabel='Coherence values')\n",
    "    ax.set_title('Topic coherence for different topics for LDA, HDP, and LSI in mesa_5000')\n",
    "    fig.savefig('num_topics.png')\n",
    "    print('computing finished')\n",
    "        \n",
    "#eval_top_k_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K parameters in HDP model \n",
    "\n",
    "def HDP_K(corpus, dictionary, texts, num_topics):\n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for k in range(5, 36, 5):\n",
    "        model = train_hdp(corpus=corpus, id2word=dictionary, T=num_topics, K=k, random_state=0)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "        \n",
    "        coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(range(5, 36, 5), coherence_values)\n",
    "    ax.set(xlabel='Num of words in each topic', ylabel='Coherence values')\n",
    "    ax.set_title('HDP model topic coherence for different words in each topic')\n",
    "    fig.savefig('HDP_num_words.png')\n",
    "    \n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "#HDP_K(corpus=corpus, dictionary=dictionary, texts=df['tokenized'], num_topics=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set onepass=False, tune power_iters parameter for LSI model\n",
    "def tune_power_iters(dictionary, corpus, num_topics, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_iter in range(20, 51, 5):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter)\n",
    "        model_list.append(model)\n",
    "\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(range(20, 51, 5), coherence_values)\n",
    "    ax.set(xlabel='Num of iterations', ylabel='Coherence values')\n",
    "    ax.set_title('LSI model topic coherence for different iterations')\n",
    "    fig.savefig('LSI_num_iters.png')\n",
    "        \n",
    "    return model_list, coherence_values\n",
    "\n",
    "# tune_power_iters(dictionary=dictionary, corpus=corpus, num_topics=6, texts=df['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set onepass=False, tune extra_samples parameter for LSI model\n",
    "def tune_extra_samples(dictionary, corpus, num_topics, num_iter, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_sample in range(100, 300, 30):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter, extra_samples=num_sample)\n",
    "        model_list.append(model)\n",
    "\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(range(100, 300, 30), coherence_values)\n",
    "    ax.set(xlabel='Num of extra samples', ylabel='Coherence values')\n",
    "    ax.set_title('LSI model topic coherence for different extra samples')\n",
    "    fig.savefig('LSI_num_extra_samples.png')\n",
    "    \n",
    "    return model_list, coherence_values\n",
    "\n",
    "#tune_extra_samples(dictionary, corpus, num_topics=6, num_iter=40, texts=df['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "passes = list(range(10, 51, 10))\n",
    "iterations = list(range(60, 110, 10))\n",
    "\n",
    "# Set eval_every to 1 and do grid search of passes and iterations for LDA model\n",
    "def LDA_grid_search(corpus, dictionary, texts, num_topics):\n",
    "    coherence_values = []\n",
    "    pass_iter_pairs = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_pass in passes:\n",
    "        for num_iter in iterations:\n",
    "            pass_iter_pairs.append((num_pass, num_iter))\n",
    "            model = train_lda(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=num_pass,\n",
    "                              iterations=num_iter, eval_every=1, random_state=0)\n",
    "            model_list.append(model)\n",
    "            coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(range(0, len(pass_iter_pairs)), coherence_values)\n",
    "    ax.set(xlabel='index of pass iter pairs', ylabel='Coherence values')\n",
    "    ax.set_title('LDA coherence values of different combinations of passes and iterations')\n",
    "    fig.savefig('LDA_num_passes_iters.png')\n",
    "    \n",
    "    return pass_iter_pairs, model_list, coherence_values\n",
    "\n",
    "\n",
    "\n",
    "# pi_pairs, models, coherence_values = LDA_grid_search(corpus=corpus, dictionary=dictionary, texts=df['tokenized'], num_topics=12)\n",
    "# print('coherence values: ', coherence_values)\n",
    "# indicies = np.argsort(np.array(coherence_values))\n",
    "# print('indices: ', indicies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhetpyaHWLUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train HDP model on businesses:  0.01051390568415324 min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>address</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>IXUwuNjy707wJNW2U4sRVg</td>\n",
       "      <td>Alessia's Ristorante Italiano</td>\n",
       "      <td>33.436137</td>\n",
       "      <td>-111.716897</td>\n",
       "      <td>5251 E Brown Rd</td>\n",
       "      <td>4.5</td>\n",
       "      <td>I like Italian food. It's probably the only fo...</td>\n",
       "      <td>[like, italian, food, food, take, home, left, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>xTW5PkLEdMBs2f2W8RGy0g</td>\n",
       "      <td>Miele's Italian and Banquet Hall</td>\n",
       "      <td>33.364794</td>\n",
       "      <td>-111.878331</td>\n",
       "      <td>2050 W Guadalupe Rd, Ste 9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I have tried this place now twice for delivery...</td>\n",
       "      <td>[tri, place, deliveri, pickup, pizza, delici, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>wctvZYbHAo8jufqAFQ457g</td>\n",
       "      <td>The Hub Grill and Bar</td>\n",
       "      <td>33.381584</td>\n",
       "      <td>-111.807068</td>\n",
       "      <td>1860 S Stapley Dr</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Went here for lunch and had the same awesome w...</td>\n",
       "      <td>[lunch, awesom, waitress, locat, time, ask, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>iJBnqweAPDTCfyMcRrG90w</td>\n",
       "      <td>Giant Hamburgers</td>\n",
       "      <td>33.406408</td>\n",
       "      <td>-111.771654</td>\n",
       "      <td>2753 E Broadway Rd, Ste 104</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Amazing amazing amazing.  Everyone, you must t...</td>\n",
       "      <td>[amaz, amaz, amaz, everyon, tri, small, place,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>iBvF9Oy9UdOrXvTlxNHyqw</td>\n",
       "      <td>La Casa de Juana</td>\n",
       "      <td>33.393456</td>\n",
       "      <td>-111.872695</td>\n",
       "      <td>1976 W Southern Ave</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I think we've got a new local favorite!\\nWe've...</td>\n",
       "      <td>[think, weve, new, local, tri, mexican, place,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>iBCMaNm_hv9IlCDa7AWPig</td>\n",
       "      <td>Mango's Mexican Cafe</td>\n",
       "      <td>33.415444</td>\n",
       "      <td>-111.833145</td>\n",
       "      <td>44 W Main St</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I love this spot. Best chips and salsa in the ...</td>\n",
       "      <td>[love, spot, best, chip, east, valley, enjoy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>i066yR2IDP4FWt6p-k9aFg</td>\n",
       "      <td>Ike's Love &amp; Sandwiches</td>\n",
       "      <td>33.390528</td>\n",
       "      <td>-111.855760</td>\n",
       "      <td>1130 W Grove Ave, Ste 110</td>\n",
       "      <td>4.5</td>\n",
       "      <td>OMG. My son went to Ike's in California and we...</td>\n",
       "      <td>[omg, son, ike, california, mesa, count, mont,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>hRFKKf8jBnn4paxnNUK1hA</td>\n",
       "      <td>Hooters</td>\n",
       "      <td>33.383454</td>\n",
       "      <td>-111.857178</td>\n",
       "      <td>1665 S Alma School Rd</td>\n",
       "      <td>2.5</td>\n",
       "      <td>It appears that this location has trouble coun...</td>\n",
       "      <td>[locat, troubl, count, go, order, wing, short,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>ihheHS4noJayWgECQpeJ_A</td>\n",
       "      <td>Teharu Sushi</td>\n",
       "      <td>33.385165</td>\n",
       "      <td>-111.687144</td>\n",
       "      <td>6638 E Superstition Springs Blvd, Ste 101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I first found out about this place by word of ...</td>\n",
       "      <td>[found, place, word, mouth, bank, month, gone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3oxnPPPU3YoxO9M1I2idg</td>\n",
       "      <td>Eklectic Pie - Mesa</td>\n",
       "      <td>33.379912</td>\n",
       "      <td>-111.806297</td>\n",
       "      <td>1859 S Stapley Dr, Ste 105-3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Usually the pizza is really good and the servi...</td>\n",
       "      <td>[pizza, good, servic, amaz, pizza, finish, hom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id                              name   latitude  \\\n",
       "89   IXUwuNjy707wJNW2U4sRVg     Alessia's Ristorante Italiano  33.436137   \n",
       "271  xTW5PkLEdMBs2f2W8RGy0g  Miele's Italian and Banquet Hall  33.364794   \n",
       "267  wctvZYbHAo8jufqAFQ457g             The Hub Grill and Bar  33.381584   \n",
       "192  iJBnqweAPDTCfyMcRrG90w                  Giant Hamburgers  33.406408   \n",
       "191  iBvF9Oy9UdOrXvTlxNHyqw                  La Casa de Juana  33.393456   \n",
       "190  iBCMaNm_hv9IlCDa7AWPig              Mango's Mexican Cafe  33.415444   \n",
       "189  i066yR2IDP4FWt6p-k9aFg           Ike's Love & Sandwiches  33.390528   \n",
       "188  hRFKKf8jBnn4paxnNUK1hA                           Hooters  33.383454   \n",
       "193  ihheHS4noJayWgECQpeJ_A                      Teharu Sushi  33.385165   \n",
       "0    -3oxnPPPU3YoxO9M1I2idg               Eklectic Pie - Mesa  33.379912   \n",
       "\n",
       "      longitude                                    address  stars  \\\n",
       "89  -111.716897                            5251 E Brown Rd    4.5   \n",
       "271 -111.878331                 2050 W Guadalupe Rd, Ste 9    4.0   \n",
       "267 -111.807068                          1860 S Stapley Dr    4.0   \n",
       "192 -111.771654                2753 E Broadway Rd, Ste 104    4.5   \n",
       "191 -111.872695                        1976 W Southern Ave    4.0   \n",
       "190 -111.833145                               44 W Main St    4.0   \n",
       "189 -111.855760                  1130 W Grove Ave, Ste 110    4.5   \n",
       "188 -111.857178                      1665 S Alma School Rd    2.5   \n",
       "193 -111.687144  6638 E Superstition Springs Blvd, Ste 101    3.0   \n",
       "0   -111.806297               1859 S Stapley Dr, Ste 105-3    4.0   \n",
       "\n",
       "                                                  text  \\\n",
       "89   I like Italian food. It's probably the only fo...   \n",
       "271  I have tried this place now twice for delivery...   \n",
       "267  Went here for lunch and had the same awesome w...   \n",
       "192  Amazing amazing amazing.  Everyone, you must t...   \n",
       "191  I think we've got a new local favorite!\\nWe've...   \n",
       "190  I love this spot. Best chips and salsa in the ...   \n",
       "189  OMG. My son went to Ike's in California and we...   \n",
       "188  It appears that this location has trouble coun...   \n",
       "193  I first found out about this place by word of ...   \n",
       "0    Usually the pizza is really good and the servi...   \n",
       "\n",
       "                                             tokenized  \n",
       "89   [like, italian, food, food, take, home, left, ...  \n",
       "271  [tri, place, deliveri, pickup, pizza, delici, ...  \n",
       "267  [lunch, awesom, waitress, locat, time, ask, su...  \n",
       "192  [amaz, amaz, amaz, everyon, tri, small, place,...  \n",
       "191  [think, weve, new, local, tri, mexican, place,...  \n",
       "190  [love, spot, best, chip, east, valley, enjoy, ...  \n",
       "189  [omg, son, ike, california, mesa, count, mont,...  \n",
       "188  [locat, troubl, count, go, order, wing, short,...  \n",
       "193  [found, place, word, mouth, bank, month, gone,...  \n",
       "0    [pizza, good, servic, amaz, pizza, finish, hom...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_name = 'mesa_5000.csv'\n",
    "query = pd.read_csv(csv_name, index_col=0)\n",
    "query = query.loc[query['name']==\"Alessia's Ristorante Italiano\"]\n",
    "query['tokenized'] = query['tokenized'].apply(eval)\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "lsi_model = train_hdp(corpus=corpus, id2word=dictionary, T=12)\n",
    "lsi_model[corpus][0]\n",
    "query_bow = lsi_model.id2word.doc2bow(query.iloc[0]['tokenized'])\n",
    "lsi_model[query_bow]\n",
    "most_sim_ids = get_most_similar_businesses(query.iloc[0]['tokenized'], corpus, dictionary, lsi_model)\n",
    "tmp = df.iloc[most_sim_ids,:]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def write_to_pickle():\n",
    "    df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "    df['tokenized'] = df['tokenized'].apply(eval)\n",
    "    dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "    pickle.dump(dictionary, open(\"mesa_dictionary.pkl\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FunctionLDA.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
