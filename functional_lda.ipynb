{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7TpVKeITKFY"
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import os\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print('Downloads Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbTRSwxgTvUC"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def initial_clean(text):\n",
    "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = text.lower() # lower case the text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "def pos(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "informative_pos = ('JJ','VB', 'NN','RBS','VBP','IN','RBR','JJR','JJS','PDT','RP','UH','FW','NNS','VBN','VBG')\n",
    "\n",
    "def remove_uninformative_pos(text):\n",
    "    tagged_words = nltk.pos_tag(text)\n",
    "    return [word for word, tag in tagged_words if tag in informative_pos]\n",
    "  \n",
    "clutter = ['food','place','good','order','great','like',\n",
    "           'service','time','go','ordered','get','love',\n",
    "           'best','come','eat','dont','tried','try','ask',\n",
    "           'nice','restaurant','ive','im','didnt']\n",
    "\n",
    "def remove_garbage(text):\n",
    "    return [word for word in text if word not in clutter]\n",
    "\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
    "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(remove_garbage(remove_uninformative_pos(remove_stop_words(initial_clean(text)))))\n",
    "\n",
    "def get_top_k_words(df, k = 10000):\n",
    "    # first get a list of all words\n",
    "    all_words = [word for item in list(df['tokenized']) for word in item]\n",
    "    \n",
    "    # use nltk fdist to get a frequency distribution of all words\n",
    "    fdist = FreqDist(all_words)\n",
    "    \n",
    "    # define a function only to keep words in the top k words\n",
    "    top_k_words, _ = zip(*fdist.most_common(k))\n",
    "    top_k_words = set(top_k_words)\n",
    "    \n",
    "    return top_k_words\n",
    "\n",
    "def keep_top_k_words(text, *top_k_words):\n",
    "    return [word for word in text if word in top_k_words]\n",
    "\n",
    "def transform_dataset(df):\n",
    "    # format the columns\n",
    "    df = df.groupby(['business_id', 'name'])['text'].apply(' '.join).reset_index()\n",
    "    df = df[df['text'].map(type) == str]\n",
    "    df.dropna(axis=0, inplace=True, subset=['text'])\n",
    "    return df\n",
    "\n",
    "def gen_tokenized_column(df):\n",
    "    # preprocess the text and business name and create new column \"tokenized\"\n",
    "    df['tokenized'] = df['text'].apply(apply_all)\n",
    "    top_k_words = get_top_k_words(df)\n",
    "    df['tokenized'] = df['tokenized'].apply(keep_top_k_words, args=(top_k_words))\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(df):\n",
    "    t1 = time.time()\n",
    "    preprocessed_df = gen_tokenized_column(transform_dataset(df))\n",
    "    t2 = time.time()\n",
    "    print(\"Time to clean and tokenize\", len(df), \"businesses' reviews:\", (t2-t1)/60, \"min\")\n",
    "    return preprocessed_df\n",
    "    \n",
    "def get_coherence_score(model, texts, dictionary):\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "def get_dictionary_corpus(data, no_below=20, no_above=0.5):\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def get_perplexity(model, corpus):\n",
    "    # a measure of how good the model is; lower the better\n",
    "    return lda_model.log_perplexity(corpus)\n",
    "\n",
    "def train_lda(corpus, id2word, chunksize=2000, num_topics=12, alpha='auto', eta='auto', passes=1, iterations=50,\n",
    "              minimum_probability=0.01, eval_every=10, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, alpha=alpha, eta=eta, \n",
    "                   chunksize=chunksize, minimum_probability=minimum_probability, passes=passes, \n",
    "                   iterations=iterations, eval_every=eval_every, random_state=random_state)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lda\n",
    "\n",
    "def train_hdp(corpus, id2word, chunksize=2000, T=150, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the hdp model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    hdp = HdpModel(corpus=corpus, id2word=id2word, T=T, chunksize=chunksize, random_state=random_state)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train HDP model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return hdp\n",
    "\n",
    "def train_lsi(corpus, id2word, num_topics=12, chunksize=2000, onepass=True, power_iters=2, extra_samples=100):\n",
    "    \"\"\"\n",
    "    This function trains the lsi model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lsi = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, chunksize=chunksize)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LSI model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lsi\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    # lets keep with the p,q notation above\n",
    "    p = query[None,:].T # take transpose\n",
    "    q = matrix.T # transpose matrix\n",
    "    m = 0.5*(p + q)\n",
    "    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))\n",
    "  \n",
    "def get_most_similar_documents(query, matrix, k=10):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    sims = jensen_shannon(query, matrix) # list of jensen shannon distances\n",
    "    indicies = sims.argsort()[:k]\n",
    "    # the top k positional index of the smallest Jensen Shannon distances and top k values based on the indices\n",
    "    return indices, sims[indicies]\n",
    "\n",
    "def get_topic_dist(model, corpus):\n",
    "    doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in model[corpus]])\n",
    "    return doc_topic_dist\n",
    "\n",
    "def get_most_similar_businesses(query_data, corpus, model):\n",
    "    query_bow = dictionary.doc2bow(query_data)\n",
    "    new_doc_dist = get_topic_dist(model, query_bow)\n",
    "    doc_dist = get_topic_dist(model, corpus)\n",
    "    most_sim_ids, most_sim_values = get_most_similar_documents(new_doc_dist, doc_dist)\n",
    "    return most_sim_ids, most_sim_values, doc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LDA\n",
    "def select_num_topics_LDA(dictionary, corpus, texts, end, label, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lda(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "    \n",
    "    plt.plot(range(start, end + 1, step), coherence_values)\n",
    "    plt.title('LDA coherence values on different number of topics')\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lda' + label + '.png')\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_num_topics_HDP(dictionary, corpus, texts, end, label, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the HDP model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, end + 1, step): \n",
    "        model = train_hdp(corpus=corpus, id2word=dictionary, T=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "            \n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "    \n",
    "    plt.plot(range(start, end + 1, step), coherence_values)\n",
    "    plt.title('HDP coherence values on different number of topics')\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('hdp' + label + '.png')\n",
    "\n",
    "    return model, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LSI\n",
    "def select_num_topics_LSI(dictionary, corpus, texts, end, label, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LSI topic models\n",
    "    coherence_values : Coherence values corresponding to the Lsi model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "    \n",
    "    plt.plot(range(start, end + 1, step), coherence_values)\n",
    "    plt.title('LSI coherence values on different number of topics')\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi' + label + '.png')\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameter top_k_words parameter num_topics\n",
    "def eval_top_k_words():\n",
    "    for fn in ('mesa_5000.csv', 'mesa_7500.csv', 'mesa_10000.csv'):\n",
    "        df = pd.read_csv(fn, index_col=0)\n",
    "        df['tokenized'] = df['tokenized'].apply(eval)\n",
    "        dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "        print('preprocessing ' + fn + ' finished')\n",
    "        select_num_topics_LDA(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=30, label=fn)\n",
    "        select_num_topics_HDP(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=30, label=fn)\n",
    "        select_num_topics_LSI(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=30, label=fn)\n",
    "        print('computing ' + fn + ' finished')\n",
    "        \n",
    "eval_top_k_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_name = 'mesa_5000'\n",
    "# df = pd.read_csv(csv_name, index_col=0)\n",
    "# df['tokenized'] = df['tokenized'].apply(eval)\n",
    "# dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "# Set onepass=False, tune power_iters parameter for LSI model\n",
    "def tune_power_iters(dictionary, corpus, num_topics, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_iter in range(3, 31, 3):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    plt.plot(range(3, 31, 3), coherence_values)\n",
    "    plt.title('LSI power_iters parameter experiment')\n",
    "    plt.xlabel(\"Num of iterations\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi_num_iters.png')\n",
    "        \n",
    "    return model_list, coherence_values\n",
    "\n",
    "tune_power_iters(dictionary, corpus, num_topics=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set onepass=False, tune extra_samples parameter for LSI model\n",
    "def tune_extra_samples(dictionary, corpus, num_topics, num_iter, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_sample in range(100, 300, 30):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter, extra_samples=num_sample)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    plt.plot(range(100, 300, 30), coherence_values)\n",
    "    plt.title('LSI extra_samples parameter experiment')\n",
    "    plt.xlabel(\"Num of extra samples\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi_extra_samples.png')\n",
    "    \n",
    "    return model_list, coherence_values\n",
    "\n",
    "tune_extra_samples(dictionary, corpus, num_topics=12, num_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "passes = list(range(10, 51, 10))\n",
    "iterations = list(range(100, 410, 60))\n",
    "\n",
    "# Set eval_every to 1 and do grid search of passes and iterations for LDA model\n",
    "def LDA_grid_search(corpus, dictionary, texts, num_topics):\n",
    "    coherence_values = []\n",
    "    pass_iter_pairs = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_pass in passes:\n",
    "        for num_iter in iterations:\n",
    "            pass_iter_pairs.append((num_pass, num_iter))\n",
    "            print(f\"--------------- pass: {num_pass}, iter: {num_iter}--------------\")\n",
    "            model = train_lda(corpus=corpus, id2word=dictionary.id2token, num_topics=num_topics, passes=num_pass,\n",
    "                              iterations=num_iter, eval_every=1, random_state=0)\n",
    "            model_list.append(model)\n",
    "            coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherence_model.get_coherence())\n",
    "            print()\n",
    "            \n",
    "    plt.plot(range(0, len(pass_iter_pairs)), coherence_values)\n",
    "    plt.title('passes and iterations to select best LDA model')\n",
    "    plt.xlabel(\"Index of pass and iteration pairs\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('passes_iters_LDA.png')\n",
    "    \n",
    "    return pass_iter_pairs, model_list, coherence_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune no_above and no_below parameters in filter_extremes method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search on T, K parameters in HDP model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try two optimized versions of LDA model: ldavowpalwabbit and ldamallet if time allows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Hellinger, Kullback–Leibler to measure distance metrics for probability distributions after selecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.016919672),\n",
       " (2, 0.086762264),\n",
       " (3, 0.032220233),\n",
       " (4, 0.23659779),\n",
       " (5, 0.022509795),\n",
       " (8, 0.01265313),\n",
       " (9, 0.030311227),\n",
       " (10, 0.17654638),\n",
       " (11, 0.37557247)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.025580758),\n",
       " (2, 0.09432872),\n",
       " (3, 0.04854446),\n",
       " (4, 0.20399308),\n",
       " (5, 0.02056593),\n",
       " (8, 0.026410503),\n",
       " (9, 0.036607433),\n",
       " (10, 0.198711),\n",
       " (11, 0.33592972)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhetpyaHWLUm"
   },
   "outputs": [],
   "source": [
    "csv_name = 'mesa_5000.csv'\n",
    "query = pd.read_csv(csv_name, index_col=0)\n",
    "query = query.loc[query['name']==\"Alessia's Ristorante Italiano\"]\n",
    "query['tokenized'] = query['tokenized'].apply(eval)\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "lsi_model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=6)\n",
    "hdp_model = train_hdp(corpus=corpus, id2word=dictionary, T=12, random_state=0)\n",
    "\n",
    "\n",
    "most_sim_ids_lsi, _, lsi_topic_dist = get_most_similar_businesses(query.iloc[0]['tokenized'], corpus, lsi_model)\n",
    "most_sim_ids_hdp, _, hdp_topic_dist = get_most_similar_businesses(query.iloc[0]['tokenized'], corpus, hdp_model)\n",
    "\n",
    "most_similar_df_lsi = df[df.index.isin(most_sim_ids_lsi)]\n",
    "most_similar_df_hdp = df[df.index.isin(most_sim_ids_hdp)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(lsi_topic_dist, open(\"processor.pkl\",\"wb\"))\n",
    "preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "# print the results\n",
    "most_similar_df = df[df.index.isin(most_sim_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zONtAIhpfIV_"
   },
   "outputs": [],
   "source": [
    "# get the ids of the most similar businesses\n",
    "# new_bow = dictionary.doc2bow()\n",
    "# new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "# most_sim_ids = get_most_similar_documents(new_doc_distribution, doc_topic_dist)\n",
    "\n",
    "\n",
    "\n",
    "# print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDBy4J73vH3B"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(doc_topic_dist,open(\"processor.pkl\",\"wb\"))\n",
    "preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "# print the results\n",
    "most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OTkrbs7zvoo"
   },
   "outputs": [],
   "source": [
    "# def process_query(preprocessed, query):\n",
    "#   # SQL to pandas DataFrame w/ query\n",
    "#   query = query.groupby(['name'])['text'].apply(' '.join).reset_index()\n",
    "#   query = query[query['text'].map(type) == str]\n",
    "#   query.dropna(axis=0, inplace=True, subset=['text'])\n",
    "#   query['tokenized'] = query['text'].apply(apply_all)\n",
    "  \n",
    "#   # read the cached pickle\n",
    "#   preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "  \n",
    "#   # get the ids of the most similar businesses\n",
    "#   new_bow = dictionary.doc2bow(query.iloc[0,2])\n",
    "#   new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "#   most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "#   # print the results\n",
    "#   most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "#   print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a24ec2668>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.arange(3, 31, 3)\n",
    "y = np.exp(x)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FunctionLDA.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
