{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import tokenize \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def read_from_file(filename):\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    return df\n",
    "\n",
    "# Data exploration\n",
    "def data_exploration(dataset):\n",
    "    dataset.isnull().sum() # all zeros\n",
    "    dataset.business_id.nunique() # 284\n",
    "    dataset.name.nunique() # 271\n",
    "    dataset.groupby('name').size().sort_values(ascending=False)\n",
    "    dataset.groupby('business_id').size().sort_values(ascending=False).head()\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    reviews_by_businessid = dataset.groupby(['business_id'])['text'].apply(' '.join).reset_index()\n",
    "    return reviews_by_businessid\n",
    "    \n",
    "def sent_preprocess(sent):\n",
    "    tokens = (word for word in word_tokenize(sent))\n",
    "    filtered_tokens = (token.lower() for token in tokens if re.match(\"^[A-Za-z-]*$\", token))\n",
    "    lemmatized_tokens = (lemmatizer.lemmatize(t, 'v') for t in filtered_tokens)\n",
    "    #final_tokens = [s for s in lemmatized_tokens if s not in stop_words]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def preprocess(reviews_by_businessid):\n",
    "    # remove head() later\n",
    "    reviews_dict = dict(zip(reviews_by_businessid.head(1).business_id, reviews_by_businessid.head(1).text))\n",
    "    sentences_dict = {}\n",
    "    processed_sent_dict = {}\n",
    "    \n",
    "    for bi, text in reviews_dict.items():\n",
    "        sentences_dict[bi] = sent_tokenize(text)\n",
    "    \n",
    "    for bi, sent_list in sentences_dict.items():\n",
    "        new_sent_list = set()\n",
    "        \n",
    "        for sent in sent_list:\n",
    "            processed_sent = sent_preprocess(sent)\n",
    "            new_sent_list.add(processed_sent)\n",
    "        \n",
    "        processed_sent_dict[bi] = new_sent_list\n",
    "    \n",
    "    return processed_sent_dict, sentences_dict\n",
    "\n",
    "def select_pos_reviews_vader(processed_reviews, original_reviews):\n",
    "    pos_reviews_dict = {}\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    for bi, sent_list in processed_reviews.items():\n",
    "        pos_sentences = set()\n",
    "        pos_scores = []\n",
    "        \n",
    "        for idx, sentence in enumerate(sent_list):\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            \n",
    "            #pos_score = ss['pos']\n",
    "            #neg_score = ss['neg']\n",
    "            #neu_score = ss['neu']\n",
    "            compound_score = ss['compound']\n",
    "            #pos_scores.append(compound_score)\n",
    "        \n",
    "            if compound_score >= 0.05:\n",
    "                original_sentence = original_reviews[bi][idx]\n",
    "                pos_sentences.add(re.sub('\\s+', ' ', original_sentence).replace(\"\\\\\", \"\"))\n",
    "                #print('original sentence: ', original_sentence)\n",
    "                #print('processed sentence; ', sentence)\n",
    "        \n",
    "        #indices = nlargest(10, range(len(pos_scores)), key=lambda idx: pos_scores[idx])\n",
    "        pos_reviews_dict[bi] = pos_sentences\n",
    "    \n",
    "    return pos_reviews_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 987 ms, sys: 79.3 ms, total: 1.07 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fn = \"csv_reviews_mesa.csv\"\n",
    "mesa = read_from_file(fn)\n",
    "\n",
    "# change assignment later\n",
    "selected_businesses = mesa\n",
    "reviews_by_businessid = clean_dataset(selected_businesses)\n",
    "\n",
    "processed_reviews, original_reviews = preprocess(reviews_by_businessid)\n",
    "res = select_pos_reviews_vader(processed_reviews, original_reviews)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
