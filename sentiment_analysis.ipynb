{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import tokenize \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from heapq import nlargest, nsmallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def read_from_file(filename):\n",
    "    return pd.read_csv(filename, index_col=0)\n",
    "\n",
    "# Data exploration\n",
    "def data_exploration(dataset):\n",
    "    dataset.isnull().sum() # all zeros\n",
    "    dataset.business_id.nunique() # 284\n",
    "    dataset.name.nunique() # 271\n",
    "    dataset.groupby('name').size().sort_values(ascending=False)\n",
    "    dataset.groupby('business_id').size().sort_values(ascending=False).head()\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    reviews_by_businessid = dataset.groupby(['business_id', 'name'])['text'].apply(' '.join).reset_index()\n",
    "    reviews_by_businessid.dropna(axis=0, inplace=True, subset=['text'])\n",
    "    return reviews_by_businessid\n",
    "    \n",
    "def sent_preprocess(sent):\n",
    "    tokens = (word for word in word_tokenize(sent))\n",
    "    filtered_tokens = (token.lower() for token in tokens if re.match(\"^[A-Za-z-]*$\", token))\n",
    "    lemmatized_tokens = (lemmatizer.lemmatize(t, 'v') for t in filtered_tokens)\n",
    "    #final_tokens = [s for s in lemmatized_tokens if s not in stop_words]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def preprocess(reviews_by_businessid):\n",
    "    reviews_dict = dict(zip(reviews_by_businessid.business_id, reviews_by_businessid.text))\n",
    "    sentences_dict = {}\n",
    "    processed_sent_dict = {}\n",
    "    \n",
    "    for bi, text in reviews_dict.items():\n",
    "        sentences_dict[bi] = sent_tokenize(text)\n",
    "    \n",
    "    for bi, sent_list in sentences_dict.items():\n",
    "        new_sent_list = []\n",
    "        \n",
    "        for sent in sent_list:\n",
    "            processed_sent = sent_preprocess(sent)\n",
    "            new_sent_list.append (processed_sent)\n",
    "        \n",
    "        processed_sent_dict[bi] = new_sent_list\n",
    "    \n",
    "    return processed_sent_dict, sentences_dict\n",
    "\n",
    "pred_pos_sent, pred_neg_sent = [], []\n",
    "\n",
    "def select_reviews_vader(processed_reviews, original_reviews):\n",
    "    pos_reviews_dict, neg_reviews_dict = {}, {}\n",
    "    \n",
    "    for bi, sent_list in processed_reviews.items():\n",
    "        sent_scores = []\n",
    "        \n",
    "        for idx, sentence in enumerate(sent_list):\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            \n",
    "            #pos_score = ss['pos']\n",
    "            #neg_score = ss['neg']\n",
    "            #neu_score = ss['neu']\n",
    "            compound_score = ss['compound']\n",
    "            sent_scores.append(compound_score)\n",
    "            if compound_score >= 0.05:\n",
    "                s1 = re.sub('\\s+', ' ', original_reviews[bi][idx])\n",
    "                pred_pos_sent.append(s1)\n",
    "            elif compound_score <= -0.05:\n",
    "                s2 = re.sub('\\s+', ' ', original_reviews[bi][idx])\n",
    "                pred_neg_sent.append(s2)\n",
    "        \n",
    "        pos_indices = nlargest(10, range(len(sent_scores)), key=lambda idx: sent_scores[idx])\n",
    "        neg_indices = nsmallest(10, range(len(sent_scores)), key=lambda idx: sent_scores[idx])\n",
    "        pos_sentences = [re.sub('\\s+', ' ', original_reviews[bi][i]).replace(\"\\\\\", \"\") for i in pos_indices]\n",
    "        neg_sentences = [re.sub('\\s+', ' ', original_reviews[bi][j]).replace(\"\\\\\", \"\") for j in neg_indices]\n",
    "        \n",
    "        pos_reviews_dict[bi], neg_reviews_dict[bi] = pos_sentences, neg_sentences\n",
    "    \n",
    "    return pos_reviews_dict, neg_reviews_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess phase completed\n",
      "sentiment classification completed\n"
     ]
    }
   ],
   "source": [
    "fn = \"clean_charlotte.csv\"\n",
    "mesa = read_from_file(fn)\n",
    "processed_reviews, original_reviews = preprocess(mesa)\n",
    "print('preprocess phase completed')\n",
    "pos_reviews_dict, neg_reviews_dict = select_reviews_vader(processed_reviews, original_reviews)\n",
    "print('sentiment classification completed')\n",
    "\n",
    "d = {'business_id': [*pos_reviews_dict], 'postive_reviews': [*pos_reviews_dict.values()],\n",
    "                'negative_reviews': [*neg_reviews_dict.values()]}\n",
    "\n",
    "selected_reviews = pd.DataFrame(data=d)\n",
    "selected_reviews\n",
    "\n",
    "#merged = pd.merge(mesa, selected_reviews, how='inner', on='business_id')\n",
    "\n",
    "#merged.to_csv(\"merged_charlotte.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess phase completed\n",
      "sentiment classification completed\n"
     ]
    }
   ],
   "source": [
    "mesa = read_from_file(\"clean_mesa.csv\")\n",
    "processed_reviews, original_reviews = preprocess(mesa)\n",
    "print('preprocess phase completed')\n",
    "pos_reviews_dict, neg_reviews_dict = select_reviews_vader(processed_reviews, original_reviews)\n",
    "print('sentiment classification completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews_df = pd.DataFrame({'reviews': pred_pos_sent, 'sentiment': [True]*len(pred_pos_sent)})\n",
    "pos_reviews_df.sample(n=150).to_csv(\"positive_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews_df = pd.DataFrame({'reviews': pred_neg_sent, 'sentiment': [False]*len(pred_neg_sent)})\n",
    "neg_reviews_df.sample(50).to_csv(\"negative_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('positive_reviews.csv', index_col=0)\n",
    "pos = pos.reset_index(drop=True)\n",
    "pos = pos.rename(columns={\"sentiment\": \"actual_sentiment\"})\n",
    "pos['predicted_sentiment'] = [True] * len(pos)\n",
    "\n",
    "neg = pd.read_csv('negative_reviews.csv', index_col=0)\n",
    "neg = neg.reset_index(drop=True)\n",
    "\n",
    "neg = neg.rename(columns={\"sentiment\": \"actual_sentiment\"})\n",
    "neg['predicted_sentiment'] = [False] * len(neg)\n",
    "\n",
    "reviews = pd.concat([pos, neg]).reset_index(drop=True)\n",
    "reviews.to_csv('eval_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv('mesa_5000.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.3"
=======
   "version": "3.5.3"
>>>>>>> 1ad65aedd7d883b40520bec755bd96834002c53b
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
