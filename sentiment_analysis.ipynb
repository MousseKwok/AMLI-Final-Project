{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import tokenize \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from heapq import nlargest, nsmallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Data exploration\n",
    "def data_exploration(dataset):\n",
    "    dataset.isnull().sum() # all zeros\n",
    "    dataset.business_id.nunique() # 284\n",
    "    dataset.name.nunique() # 271\n",
    "    dataset.groupby('name').size().sort_values(ascending=False)\n",
    "    dataset.groupby('business_id').size().sort_values(ascending=False).head()\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    reviews_by_businessid = dataset.groupby(['business_id', 'name'])['text'].apply(' '.join).reset_index()\n",
    "    reviews_by_businessid.dropna(axis=0, inplace=True, subset=['text'])\n",
    "    return reviews_by_businessid\n",
    "    \n",
    "def sent_preprocess(sent):\n",
    "    tokens = (word for word in word_tokenize(sent))\n",
    "    filtered_tokens = (token.lower() for token in tokens if re.match(\"^[A-Za-z-]*$\", token))\n",
    "    lemmatized_tokens = (lemmatizer.lemmatize(t, 'v') for t in filtered_tokens)\n",
    "    #final_tokens = [s for s in lemmatized_tokens if s not in stop_words]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def preprocess(reviews_by_businessid):\n",
    "    reviews_dict = dict(zip(reviews_by_businessid.business_id, reviews_by_businessid.text))\n",
    "    sentences_dict = {}\n",
    "    processed_sent_dict = {}\n",
    "    \n",
    "    for bi, text in reviews_dict.items():\n",
    "        sentences_dict[bi] = sent_tokenize(text)\n",
    "    \n",
    "    for bi, sent_list in sentences_dict.items():\n",
    "        new_sent_list = []\n",
    "        \n",
    "        for sent in sent_list:\n",
    "            processed_sent = sent_preprocess(sent)\n",
    "            new_sent_list.append (processed_sent)\n",
    "        \n",
    "        processed_sent_dict[bi] = new_sent_list\n",
    "    \n",
    "    return processed_sent_dict, sentences_dict\n",
    "\n",
    "pred_pos_sent, pred_neg_sent = [], []\n",
    "\n",
    "def select_reviews_vader(processed_reviews, original_reviews):\n",
    "    reviews_dict, stats_dict = {}, {}\n",
    "    \n",
    "    for bi, sent_list in processed_reviews.items():\n",
    "        sent_scores = []\n",
    "        pos_count, neg_count = 0, 0\n",
    "        \n",
    "        for idx, sentence in enumerate(sent_list):\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            \n",
    "            compound_score = ss['compound']\n",
    "            sent_scores.append(compound_score)\n",
    "            \n",
    "            if compound_score >= 0.05:\n",
    "                pos_count = pos_count + 1\n",
    "                s1 = re.sub('\\s+', ' ', original_reviews[bi][idx])\n",
    "                pred_pos_sent.append(s1)\n",
    "                \n",
    "            elif compound_score <= -0.05:\n",
    "                neg_count = neg_count + 1\n",
    "                s2 = re.sub('\\s+', ' ', original_reviews[bi][idx])\n",
    "                pred_neg_sent.append(s2)\n",
    "        \n",
    "        stats_dict[bi] = (pos_count, neg_count, pos_count/(pos_count+neg_count), neg_count/(pos_count+neg_count))\n",
    "        \n",
    "        pos_indices = nlargest(5, range(len(sent_scores)), key=lambda idx: sent_scores[idx])\n",
    "        neg_indices = nsmallest(5, range(len(sent_scores)), key=lambda idx: sent_scores[idx])\n",
    "        \n",
    "        pos_sentences = [re.sub('\\s+', ' ', original_reviews[bi][i]).replace(\"\\\\\", \"\")\n",
    "                         for i in pos_indices if sent_scores[i] >= 0.05]\n",
    "        neg_sentences = [re.sub('\\s+', ' ', original_reviews[bi][j]).replace(\"\\\\\", \"\")\n",
    "                         for j in neg_indices if sent_scores[j] <= 0.05]\n",
    "        \n",
    "        reviews_dict[bi] = (pos_sentences, neg_sentences)\n",
    "    \n",
    "    return reviews_dict, stats_dict\n",
    "\n",
    "def get_sentiments(fn):\n",
    "    pred_pos_sent.clear()\n",
    "    pred_neg_sent.clear()\n",
    "    df = pd.read_csv(fn, index_col=0)\n",
    "    processed_reviews, original_reviews = preprocess(df)\n",
    "    print('preprocess phase completed')\n",
    "    reviews_dict, stats_dict = select_reviews_vader(processed_reviews, original_reviews)\n",
    "    print('sentiment classification completed')\n",
    "    \n",
    "    business_ids = list(reviews_dict.keys())\n",
    "    reviews = list(reviews_dict.values())\n",
    "    stats = list(stats_dict.values())\n",
    "    \n",
    "    businessid_df = pd.DataFrame({'business_id': business_ids})\n",
    "    reviews_df = pd.DataFrame(reviews, columns=['postive_reviews', 'negative_reviews'])\n",
    "    stats_df = pd.DataFrame(stats, columns=['num_pos', 'num_neg', 'pos_ratio', 'neg_ratio'])\n",
    "    return pd.concat([businessid_df, reviews_df, stats_df], axis=1)\n",
    "\n",
    "# Sample sample_num_pos positive reviews and sample_num_neg negative reviews for evaluation\n",
    "def sample_reviews(fn, sample_num_pos, sample_num_neg):\n",
    "    pred_pos_sent.clear()\n",
    "    pred_neg_sent.clear()\n",
    "    df = pd.read_csv(fn, index_col=0)\n",
    "    processed_reviews, original_reviews = preprocess(df)\n",
    "    print('preprocess phase completed')\n",
    "    select_reviews_vader(processed_reviews, original_reviews)\n",
    "    print('sentiment classification completed')\n",
    "    \n",
    "    pos_reviews_df = pd.DataFrame({'reviews': pred_pos_sent, 'actual_sentiment': [True]*len(pred_pos_sent),\n",
    "                                  'predicted_sentiment': [True]*len(pred_pos_sent)}) \n",
    "    neg_reviews_df = pd.DataFrame({'reviews': pred_neg_sent, 'actual_sentiment': [False]*len(pred_neg_sent),\n",
    "                                  'predicted_sentiment': [True]*len(pred_neg_sent)})\n",
    "    \n",
    "    sampled_pos_reviews = pos_reviews_df.sample(n=sample_num_pos)\n",
    "    sampled_neg_reviews = neg_reviews_df.sample(n=sample_num_neg)\n",
    "\n",
    "    reviews = pd.concat([sampled_pos_reviews, sampled_neg_reviews]).reset_index(drop=True)\n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess phase completed\n",
      "sentiment classification completed\n"
     ]
    }
   ],
   "source": [
    "sampled_reviews = sample_reviews('mesa_5000.csv', sample_num_pos=150, sample_num_neg=50)\n",
    "sampled_reviews.to_csv('eval_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_names = [\"pittsburgh_reviews\", \"mesa_reviews\", \"charlotte_reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess phase completed\n",
      "sentiment classification completed\n",
      "preprocess phase completed\n",
      "sentiment classification completed\n",
      "preprocess phase completed\n",
      "sentiment classification completed\n"
     ]
    }
   ],
   "source": [
    "for csv in csv_names:\n",
    "    sentiments = get_sentiments(\"%s_cleaned.csv\" % csv)\n",
    "    original_df = pd.read_csv(\"%s.csv\" % csv)\n",
    "    final = pd.merge(original_df,sentiments,how = \"left\",on = \"business_id\")\n",
    "    final.to_csv(\"%s_final.csv\" % csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
