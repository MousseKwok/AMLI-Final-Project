{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import tokenize \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from heapq import nlargest\n",
    "from heapq import nsmallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "num_businesses = 5\n",
    "\n",
    "def read_from_file(filename):\n",
    "    return pd.read_csv(filename, index_col=0)\n",
    "\n",
    "# Data exploration\n",
    "def data_exploration(dataset):\n",
    "    dataset.isnull().sum() # all zeros\n",
    "    dataset.business_id.nunique() # 284\n",
    "    dataset.name.nunique() # 271\n",
    "    dataset.groupby('name').size().sort_values(ascending=False)\n",
    "    dataset.groupby('business_id').size().sort_values(ascending=False).head()\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    reviews_by_businessid = dataset.groupby(['business_id', 'name'])['text'].apply(' '.join).reset_index()\n",
    "    return reviews_by_businessid\n",
    "    \n",
    "def sent_preprocess(sent):\n",
    "    tokens = (word for word in word_tokenize(sent))\n",
    "    filtered_tokens = (token.lower() for token in tokens if re.match(\"^[A-Za-z-]*$\", token))\n",
    "    lemmatized_tokens = (lemmatizer.lemmatize(t, 'v') for t in filtered_tokens)\n",
    "    #final_tokens = [s for s in lemmatized_tokens if s not in stop_words]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def get_names(reviews_by_businessid):\n",
    "    return reviews_by_businessid.head(num_businesses).name\n",
    "    \n",
    "def preprocess(reviews_by_businessid):\n",
    "    reviews_dict = dict(zip(reviews_by_businessid.head(num_businesses).business_id, reviews_by_businessid.head(num_businesses).text))\n",
    "    sentences_dict = {}\n",
    "    processed_sent_dict = {}\n",
    "    \n",
    "    for bi, text in reviews_dict.items():\n",
    "        sentences_dict[bi] = sent_tokenize(text)\n",
    "    \n",
    "    for bi, sent_list in sentences_dict.items():\n",
    "        new_sent_list = []\n",
    "        \n",
    "        for sent in sent_list:\n",
    "            processed_sent = sent_preprocess(sent)\n",
    "            new_sent_list.append (processed_sent)\n",
    "        \n",
    "        processed_sent_dict[bi] = new_sent_list\n",
    "    \n",
    "    return processed_sent_dict, sentences_dict\n",
    "\n",
    "def select_reviews_vader(processed_reviews, original_reviews):\n",
    "    pos_reviews_dict, neg_reviews_dict = {}, {}\n",
    "    \n",
    "    for bi, sent_list in processed_reviews.items():\n",
    "        sent_scores = []\n",
    "        \n",
    "        for idx, sentence in enumerate(sent_list):\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            \n",
    "            #pos_score = ss['pos']\n",
    "            #neg_score = ss['neg']\n",
    "            #neu_score = ss['neu']\n",
    "            compound_score = ss['compound']\n",
    "            sent_scores.append(compound_score)\n",
    "        \n",
    "        pos_indices = nlargest(10, range(len(sent_scores)), key=lambda idx: sent_scores[idx])\n",
    "        neg_indices = nsmallest(10, range(len(sent_scores)), key=lambda idx: sent_scores[idx])\n",
    "        pos_sentences = [re.sub('\\s+', ' ', original_reviews[bi][i]).replace(\"\\\\\", \"\") for i in pos_indices]\n",
    "        neg_sentences = [re.sub('\\s+', ' ', original_reviews[bi][j]).replace(\"\\\\\", \"\") for j in neg_indices]\n",
    "        \n",
    "        pos_reviews_dict[bi], neg_reviews_dict[bi] = pos_sentences, neg_sentences\n",
    "    \n",
    "    return pos_reviews_dict, neg_reviews_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>postive_reviews</th>\n",
       "      <th>negative_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3oxnPPPU3YoxO9M1I2idg</td>\n",
       "      <td>Eklectic Pie - Mesa</td>\n",
       "      <td>[Love love love love love., The atmosphere was...</td>\n",
       "      <td>[The only thing that lost points is the staff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-oSII3bw90cvyLmgsHgmpg</td>\n",
       "      <td>Mokis Hawaiian Grill</td>\n",
       "      <td>[Really good, really authentic Hawaiian faire ...</td>\n",
       "      <td>[I leave full, but unfulfilled, and again I am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-sNi7U9seVfCr8T8nkWd_w</td>\n",
       "      <td>Rumbi Island Grill</td>\n",
       "      <td>[This restaurant had a beautiful interior desi...</td>\n",
       "      <td>[Cheap cuts of meat on the steak and chicken, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01xTdrNUuTOAyH7NaRWcUA</td>\n",
       "      <td>Mellow Mushroom</td>\n",
       "      <td>[Great vibes super laid back, Robert the serve...</td>\n",
       "      <td>[So, since everything about this dining \"Exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09psTuUYhUMA2ZRzQlm30Q</td>\n",
       "      <td>Five Guys</td>\n",
       "      <td>[Now to be honest, I don't eat burgers that of...</td>\n",
       "      <td>[Let me just make a list of WTF items: Wicked ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                  name  \\\n",
       "0  -3oxnPPPU3YoxO9M1I2idg   Eklectic Pie - Mesa   \n",
       "1  -oSII3bw90cvyLmgsHgmpg  Mokis Hawaiian Grill   \n",
       "2  -sNi7U9seVfCr8T8nkWd_w    Rumbi Island Grill   \n",
       "3  01xTdrNUuTOAyH7NaRWcUA       Mellow Mushroom   \n",
       "4  09psTuUYhUMA2ZRzQlm30Q             Five Guys   \n",
       "\n",
       "                                     postive_reviews  \\\n",
       "0  [Love love love love love., The atmosphere was...   \n",
       "1  [Really good, really authentic Hawaiian faire ...   \n",
       "2  [This restaurant had a beautiful interior desi...   \n",
       "3  [Great vibes super laid back, Robert the serve...   \n",
       "4  [Now to be honest, I don't eat burgers that of...   \n",
       "\n",
       "                                    negative_reviews  \n",
       "0  [The only thing that lost points is the staff ...  \n",
       "1  [I leave full, but unfulfilled, and again I am...  \n",
       "2  [Cheap cuts of meat on the steak and chicken, ...  \n",
       "3  [So, since everything about this dining \"Exper...  \n",
       "4  [Let me just make a list of WTF items: Wicked ...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "fn = \"csv_reviews_mesa.csv\"\n",
    "mesa = read_from_file(fn)\n",
    "\n",
    "# change assignment later\n",
    "selected_businesses = mesa\n",
    "reviews_by_businessid = clean_dataset(selected_businesses)\n",
    "\n",
    "id_names = get_names(reviews_by_businessid)\n",
    "\n",
    "processed_reviews, original_reviews = preprocess(reviews_by_businessid)\n",
    "pos_reviews_dict, neg_reviews_dict = select_reviews_vader(processed_reviews, original_reviews)\n",
    "\n",
    "d = {'business_id': [*pos_reviews_dict], 'name': id_names, 'postive_reviews': [*pos_reviews_dict.values()],\n",
    "                'negative_reviews': [*neg_reviews_dict.values()]}\n",
    "\n",
    "selected_reviews = pd.DataFrame(data=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
