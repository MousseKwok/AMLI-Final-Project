{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EROb0HG7ZCs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads Complete\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re, time, gensim\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import models, corpora, similarities\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from nltk import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print('Downloads Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GcitrQZZ7ZCv"
   },
   "outputs": [],
   "source": [
    "def initial_clean(text):\n",
    "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = text.lower() # lower case the text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "\n",
    "def pos(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "informative_pos = ('JJ','VB', 'NN','RBS','VBP','IN','RBR','JJR','JJS','PDT','RP','UH','FW','NNS','VBN','VBG')\n",
    "def select_informative_pos(text):\n",
    "    tagged_words = nltk.pos_tag(text)\n",
    "    return [word for word, tag in tagged_words if tag in informative_pos]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
    "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(select_informative_pos(remove_stop_words(initial_clean(text))))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCZcd7G47ZCx"
   },
   "outputs": [],
   "source": [
    "csv_names = [\"pittsburgh_reviews\", \"mesa_reviews\", \"charlotte_reviews\"]\n",
    "k = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPTPHXv2-YV3"
   },
   "outputs": [],
   "source": [
    "def preprocess(name):\n",
    "    df = pd.read_csv(name)\n",
    "    df = df.groupby(['business_id','name','latitude','longitude','address','stars','is_open'])['text'].apply(' '.join).reset_index()\n",
    "    df = df[df['text'].map(type) == str]\n",
    "    df.dropna(axis=0, inplace=True, subset=['text'])\n",
    "\n",
    "    # preprocess the text and business name and create new column \"tokenized\"\n",
    "    t1 = time.time()\n",
    "    df['tokenized'] = df['text'].apply(apply_all)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to clean and tokenize\", len(df), \"businesses' reviews:\", (t2-t1)/60, \"min\")\n",
    "\n",
    "    # use nltk fdist to get a frequency distribution of all words\n",
    "    all_words = [word for item in list(df['tokenized']) for word in item]\n",
    "    fdist = FreqDist(all_words)\n",
    "\n",
    "    #only keep words in the top k words\n",
    "    top_k_words,_ = zip(*fdist.most_common(k))\n",
    "    top_k_words = set(top_k_words)\n",
    "    \n",
    "    def keep_top_k_words(text):\n",
    "        return [word for word in text if word in top_k_words]\n",
    "    \n",
    "    df['tokenized'] = df['tokenized'].apply(keep_top_k_words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpS-w4un7ZC6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean and tokenize 442 businesses' reviews: 12.430062051614126 min\n",
      "Time to clean and tokenize 284 businesses' reviews: 5.712130137284597 min\n",
      "Time to clean and tokenize 580 businesses' reviews: 15.177245795726776 min\n"
     ]
    }
   ],
   "source": [
    "for name in csv_names:\n",
    "  df = preprocess(\"%s.csv\" % name)\n",
    "  csv = df.to_csv(\"%s.csv\" % name)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
